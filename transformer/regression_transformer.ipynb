{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b7955b",
   "metadata": {},
   "source": [
    "# Toy Problem - Transformer Application to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50377cbf",
   "metadata": {},
   "source": [
    "https://github.com/oliverguhr/transformer-time-series-prediction/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec12fa0",
   "metadata": {},
   "source": [
    "Description: This example is from the repo above. It contains 2 PyTorch models for a transformer-based time series prediction. The dataset is stored in ./daily-min-temperatures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import time \n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb50495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = source sequence length\n",
    "# T = target sequence length \n",
    "# N = batch size \n",
    "# E = feature number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43280924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 \n",
    "output_window = 5\n",
    "batch_size = 10 \n",
    "device = torch.device(\"curda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8380cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # define a pytorch module that inherits nn.Module\n",
    "    \"\"\"\n",
    "    Positional encoding layer for transformer model. \n",
    "\n",
    "    Layer injects info about the relative or absolute position of the sequence, without adding learnable parameters. \n",
    "\n",
    "    Uses sin and cos fcns of different frequencies to encode position info. \n",
    "\n",
    "    Args: \n",
    "        d_model (int): dimension of the embedding space \n",
    "        max_len (int, optional): max sequence length supported. Default is 5000 \n",
    "\n",
    "    Attriutes: \n",
    "        pe (Tensor): Fixed positional encoding matrix of space (max_len, 1, d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # the \"Attributes\" part documents the instance variables inside the __init__\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000): # creates init method \n",
    "        super(PositionalEncoding, self).__init__() # super() lets us avoid referring to the base class explicitly\n",
    "        # https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods\n",
    "        pe = torch.zeros(max_len, d_model) # create empty matrix of shape max_len X d_model to hold the positional encodings\n",
    "        # row: position i.e. 0, 1, 2, \n",
    "        # column: dim of the embedding \n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # vector of positions [0, 1, 2, 3, ..., 4999]\n",
    "        # unsqueeze reshapes vector from [max_len,] to [max_len, 1] to enable broadcasting \n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # division term for the sin and cos fcns \n",
    "        # torch.arange(0, d_model, 2).float(): starts at 0, ends at d_model, step size = 2 \n",
    "        # -ln(10000)\n",
    "        # torch.exp = exp \n",
    "        # this comes from \"Attention is all you need\" paper where sin and cos fcns of different frequencies are used where each dimension of the positional encoding corresponds to a sine\n",
    "        # PE at dim i = PE_(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        # PE_(pos, 2i+1) = cos(pos/10000^(2i/d_model)) \n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # at even indices: sin(position * frequency)\n",
    "    \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # at odd indices: cos(position * frequency)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        # pe.unsqueeze(0) == adds a batch dimension so the shape becomes: [1, max_len, d_model]\n",
    "        # .transpose(0,1) == swaps the first and second dimensions such that the new shape is [max_len, 1, d_model]\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        # saving pe tensor. Tensor which is not a parameter, but should be part of the module's state. Used for tensors that need to be on the same device as the module. \n",
    "        # it's a fixed tensor stored with the model and moved to the GPU/CPU automatically \n",
    "        # this is NOT updated during backprop \n",
    "\n",
    "    def forward(self, x): # during the forward pass, x is the input with shape [sequence length, batch_size, d_model]\n",
    "        return x + self.pe[:x.size(0), :] # add the pe for the len of the input, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35741f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USE OF POSITIONAL ENCODING\n",
    "positional_encoder = PositionalEncoding(d_model = 512)\n",
    "sample_x = torch.randn(100, 32, 512) # tensor filled with random numbers from a standard normal distribution of shape [100, 32, 512]\n",
    "sample_encode = positional_encoder(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ff6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -1.1524, -0.2506],\n",
      "         [-0.5461, -0.6302, -0.6347],\n",
      "         [-1.0841, -0.1287, -0.6811]],\n",
      "\n",
      "        [[-0.5518,  1.5398,  1.0036],\n",
      "         [-0.4424,  0.2087,  0.0160],\n",
      "         [ 1.2970, -0.4725,  0.3149]],\n",
      "\n",
      "        [[-0.9780,  0.6038, -1.7178],\n",
      "         [-0.3399, -0.2990,  1.8007],\n",
      "         [ 0.6786,  0.5225, -0.0246]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_x[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63089d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -0.1524, -0.2506],\n",
      "         [-0.5461,  0.3698, -0.6347],\n",
      "         [-1.0841,  0.8713, -0.6811]],\n",
      "\n",
      "        [[ 0.2896,  2.0801,  1.8254],\n",
      "         [ 0.3990,  0.7490,  0.8379],\n",
      "         [ 2.1385,  0.0678,  1.1368]],\n",
      "\n",
      "        [[-0.0687,  0.1877, -0.7814],\n",
      "         [ 0.5694, -0.7151,  2.7371],\n",
      "         [ 1.5879,  0.1063,  0.9118]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_encode[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f201e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module): # defines a model class \n",
    "    def __init__(self, feature_size=250, num_layers=1, dropout=0.1): # initialize model \n",
    "        # feature size: input embedding dimension \n",
    "        # num_layers: transformer layers to stack \n",
    "        # dropout: dropout rate for regularization \n",
    "\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer' # string label \n",
    "        self.src_mask = None # used for sequence masking - important in autoregressive tasks like time-series and language modeling\n",
    "        self.pos_encoder = PositionalEncoding(feature_size) # giving position awareness to the input embeddings before processed by the transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout) # one transformer encoder layer with 10 heads \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers) # stack multiple copies of the encoder layer \n",
    "        self.decoder = nn.Linear(feature_size, 1) # maps the feature size to 1 (in time series forecasting, this maps one number per position)\n",
    "        self.init_weights() # calls init_weights methods to initialize the weights \n",
    "\n",
    "    def init_weights(self): \n",
    "        initrange = 0.1 # define a small numer to set range for random initialization \n",
    "        self.decoder.bias.data.zero_() # bias is the additive constant inside Linear layers. This overwrites all biases to 0 so that only weights matter. Random biases could introduce unwanted drift right at the start so we initialize at 0\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange) # initialize weights between -0.1 and 0.1. Smaller weights help stabilize training early on \n",
    "\n",
    "    def forward(self,src): # the forward pass for the model \n",
    "\n",
    "        # because this is a causal task (like time series or language modeling), we must prevent tokens from seeing the future, so we implement this mask to avoid attention to future tokens\n",
    "        # the mask is a square matrix of size (seq_length, seq_length)\n",
    "\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) !=len(src): # check if a new source mask needs to be created (or a new one)\n",
    "            # if self.src_mask is None then no mask was created yet\n",
    "            #if self.src_mask.size(0) != len(src), the input seq len has changed since the previous iteration therefore we need one of a new size \n",
    "\n",
    "            device = src.device # get the device where the input tensor lives to ensure the mask is on the same device \n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device) \n",
    "            # generate causal mask of len(src) such that the token can attend only to itself and earlier tokens\n",
    "            self.src_mask = mask \n",
    "\n",
    "        src = self.pos_encoder(src) # apply positional encoding to the input embeddings \n",
    "        output = self.transformer_encoder(src, self.src_mask) # pass position-encoded input into the stacked transformer encoder layers \n",
    "        # this is where mlti-head self-attention happens \n",
    "        # each token attends to the previous ones bc of the mask \n",
    "\n",
    "        output = self.decoder(output)\n",
    "        # apply Linear layer to every position \n",
    "        # turn the feature_size vector to a scalar \n",
    "        # this is the model's final prediction at each time step or token \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz): \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1) # create upper triangular matrix of ones, then trnaspose flips it to make lower triangle of 1s \n",
    "        mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0)) # convert 1s/0s into attention scores. mask == 0 ==> future positions, mask == 1 ==> self and past \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda7300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare sequence data for training model\n",
    "\n",
    "def create_inout_sequences(input_data, tw): \n",
    "    # function: slices a time series dataset into overlapping i/o sequences for training \n",
    "    # input_data: 1D array (the time series)\n",
    "    # tw: time window (the sequence length)\n",
    "    # output: list of (input sequence, label sequence) pairs \n",
    "\n",
    "    inout_seq = [] # preallocate memory to hold the (input, label) pairs \n",
    "    L = len(input_data) # stores the length of the sequence \n",
    "    for i in range(L-tw): # loop through the sequence to get all sliding windows of length tw\n",
    "        # stop at L-tw to ensure input_data[i:i+tw] stays within the length of the input_data \n",
    "\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window], output_window * [0])\n",
    "        # input_data[i:i+tw]: gives a window of length tw \n",
    "        # [:-output_window]: removes the last output_window values - therefore we're making the last value(s) we want the model to predict \n",
    "        # output_window * [0]: appends 0s at the end for the same length removed\n",
    "\n",
    "        train_label = input_data[i:i+tw]\n",
    "        # this is the ground truth - the full unmasked windwo including the parts zeroed out \n",
    "\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "        # saves tuple of the masked input and the ground label \n",
    "\n",
    "    return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates and prepares synthetic time series data for the transformer, including normalizaton and splitting into training and tests sets \n",
    "def get_data(): \n",
    "\n",
    "    # generate synthetic data of a continuous signal \n",
    "    time = np.arange(0, 400, 0.1) # len = 4000 \n",
    "    amplitude = np.sin(time) + np.sin(time * 0.05) + np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
    "    # np.sin(time): main wave \n",
    "    # np.sin(time*0.05): low-frequency drift\n",
    "    # np.sin(time*0.12): modulated noise \n",
    "\n",
    "\n",
    "    # scaling the data \n",
    "    from sklearn.prepreprocessing import MinMaxScaler \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    amplitude = scaler.fit_transformer(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    # reshape(-1, 1): turns a 1D array into a 2D shape expected by MinMaxScaler, then .reshape(-1) flattens it again \n",
    "\n",
    "    # data splitting \n",
    "    samples = 2800 # 70% of 4000\n",
    "    train_data = amplitude[:samples]\n",
    "    test_data = amplitude[samples:]\n",
    "\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce2d6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, batch_size): # extracts a mini-batch of input/label pairs from the training or test set (the source), starting at position i, and prepares the right shape for the model input\n",
    "    seq_len = min(batch_size, len(source) - 1 - i) # calculates how many items to include in the current batch - if you're near the end of the dataset, you may not have enough for a full batch so you'll take whatever is left\n",
    "    data = source[i:i+seq_len] # slices the dataset from i to i+seq_len\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_windwow, 1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data): \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        if calculate_loss_over_all_values: \n",
    "            loss = criterion(output, targets)\n",
    "        else: \n",
    "            loss = criterion(output[-output_window:], target[-output_window:])\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df8cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eca47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d76c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e09f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b88ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346af34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626b369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884bc7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b570e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d979cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8589b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:256: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "<>:256: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_29902/4196323605.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_2634bauad6/croot/libtorch_1744642078920/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.FloatTensor(inout_seq)\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_29902/4196323605.py:256: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "  if(epoch % 10 is 0):\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:536: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches | lr 0.005000 | 336.11 ms | loss 5.58830 | ppl   267.28\n",
      "| epoch   1 |   106/  269 batches | lr 0.005000 | 396.79 ms | loss 0.17904 | ppl     1.20\n",
      "| epoch   1 |   159/  269 batches | lr 0.005000 | 313.90 ms | loss 0.13650 | ppl     1.15\n",
      "| epoch   1 |   212/  269 batches | lr 0.005000 | 312.80 ms | loss 0.12623 | ppl     1.13\n",
      "| epoch   1 |   265/  269 batches | lr 0.005000 | 321.68 ms | loss 0.10194 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 92.92s | valid loss 0.44007 | valid ppl     1.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches | lr 0.004802 | 291.03 ms | loss 0.11000 | ppl     1.12\n",
      "| epoch   2 |   106/  269 batches | lr 0.004802 | 282.80 ms | loss 0.06319 | ppl     1.07\n",
      "| epoch   2 |   159/  269 batches | lr 0.004802 | 323.15 ms | loss 0.09604 | ppl     1.10\n",
      "| epoch   2 |   212/  269 batches | lr 0.004802 | 370.58 ms | loss 0.08289 | ppl     1.09\n",
      "| epoch   2 |   265/  269 batches | lr 0.004802 | 354.74 ms | loss 0.07363 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 90.68s | valid loss 0.28095 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches | lr 0.004706 | 484.22 ms | loss 0.07195 | ppl     1.07\n",
      "| epoch   3 |   106/  269 batches | lr 0.004706 | 453.46 ms | loss 0.04164 | ppl     1.04\n",
      "| epoch   3 |   159/  269 batches | lr 0.004706 | 432.89 ms | loss 0.03173 | ppl     1.03\n",
      "| epoch   3 |   212/  269 batches | lr 0.004706 | 497.14 ms | loss 0.02321 | ppl     1.02\n",
      "| epoch   3 |   265/  269 batches | lr 0.004706 | 450.31 ms | loss 0.01443 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 127.60s | valid loss 0.48363 | valid ppl     1.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches | lr 0.004612 | 488.88 ms | loss 0.02671 | ppl     1.03\n",
      "| epoch   4 |   106/  269 batches | lr 0.004612 | 448.51 ms | loss 0.01258 | ppl     1.01\n",
      "| epoch   4 |   159/  269 batches | lr 0.004612 | 487.53 ms | loss 0.01801 | ppl     1.02\n",
      "| epoch   4 |   212/  269 batches | lr 0.004612 | 415.19 ms | loss 0.01710 | ppl     1.02\n",
      "| epoch   4 |   265/  269 batches | lr 0.004612 | 424.65 ms | loss 0.01507 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 124.77s | valid loss 0.49670 | valid ppl     1.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches | lr 0.004520 | 356.19 ms | loss 0.03161 | ppl     1.03\n",
      "| epoch   5 |   106/  269 batches | lr 0.004520 | 388.59 ms | loss 0.01844 | ppl     1.02\n",
      "| epoch   5 |   159/  269 batches | lr 0.004520 | 372.84 ms | loss 0.02231 | ppl     1.02\n",
      "| epoch   5 |   212/  269 batches | lr 0.004520 | 450.77 ms | loss 0.01717 | ppl     1.02\n",
      "| epoch   5 |   265/  269 batches | lr 0.004520 | 459.87 ms | loss 0.01624 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 112.04s | valid loss 0.49387 | valid ppl     1.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches | lr 0.004429 | 340.54 ms | loss 0.04098 | ppl     1.04\n",
      "| epoch   6 |   106/  269 batches | lr 0.004429 | 426.04 ms | loss 0.01458 | ppl     1.01\n",
      "| epoch   6 |   159/  269 batches | lr 0.004429 | 384.50 ms | loss 0.01650 | ppl     1.02\n",
      "| epoch   6 |   212/  269 batches | lr 0.004429 | 408.88 ms | loss 0.01803 | ppl     1.02\n",
      "| epoch   6 |   265/  269 batches | lr 0.004429 | 364.30 ms | loss 0.01673 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 106.52s | valid loss 0.43931 | valid ppl     1.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches | lr 0.004341 | 454.48 ms | loss 0.04923 | ppl     1.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 253\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    252\u001b[39m     epoch_start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m(epoch % \u001b[32m10\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[32m0\u001b[39m):\n\u001b[32m    257\u001b[39m         val_loss = plot_and_loss(model, val_data,epoch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_data)\u001b[39m\n\u001b[32m    140\u001b[39m data, targets = get_batch(train_data, i,batch_size)\n\u001b[32m    141\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m calculate_loss_over_all_values:\n\u001b[32m    145\u001b[39m     loss = criterion(output, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mTransAm.forward\u001b[39m\u001b[34m(self, src)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m.src_mask = mask\n\u001b[32m     72\u001b[39m src = \u001b[38;5;28mself\u001b[39m.pos_encoder(src)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, self.src_mask)\u001b[39;00m\n\u001b[32m     74\u001b[39m output = \u001b[38;5;28mself\u001b[39m.decoder(output)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:511\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    508\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    519\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:906\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    902\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    903\u001b[39m         x\n\u001b[32m    904\u001b[39m         + \u001b[38;5;28mself\u001b[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001b[32m    905\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/module.py:1732\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1729\u001b[39m             tracing_state.pop_scope()\n\u001b[32m   1730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1733\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1734\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "input_window = 100\n",
    "output_window = 5\n",
    "batch_size = 10 # batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "       \n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "def get_data():\n",
    "    time        = np.arange(0, 400, 0.1)\n",
    "    amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
    "    \n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    \n",
    "    sampels = 2800\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment.. \n",
    "    train_sequence = create_inout_sequences(train_data,input_window)\n",
    "    train_sequence = train_sequence[:-output_window] #todo: fix hack?\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,input_window)\n",
    "    test_data = test_data[:-output_window] #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device),test_data.to(device)\n",
    "\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i,batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)        \n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "    pyplot.plot(test_result-truth,color=\"green\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "    \n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.close()\n",
    "        \n",
    "# entweder ist hier ein fehler im loss oder in der train methode, aber die ergebnisse sind unterschiedlich \n",
    "# auch zu denen der predict_future\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).cpu().item()            \n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "train_data, val_data = get_data()\n",
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    \n",
    "    if(epoch % 10 is 0):\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() \n",
    "\n",
    "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number) \n",
    "#out = model(src)\n",
    "#\n",
    "#print(out)\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e1786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
