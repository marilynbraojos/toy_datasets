{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b7955b",
   "metadata": {},
   "source": [
    "# Toy Problem - Transformer Application to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50377cbf",
   "metadata": {},
   "source": [
    "https://github.com/oliverguhr/transformer-time-series-prediction/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec12fa0",
   "metadata": {},
   "source": [
    "Description: This example is from the repo above. It contains 2 PyTorch models for a transformer-based time series prediction. The dataset is stored in ./daily-min-temperatures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import time \n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb50495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = source sequence length\n",
    "# T = target sequence length \n",
    "# N = batch size \n",
    "# E = feature number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43280924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 \n",
    "output_window = 5\n",
    "batch_size = 10 \n",
    "lr = 0.005\n",
    "epochs = 100\n",
    "device = torch.device(\"curda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8380cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # define a pytorch module that inherits nn.Module\n",
    "    \"\"\"\n",
    "    Positional encoding layer for transformer model. \n",
    "\n",
    "    Layer injects info about the relative or absolute position of the sequence, without adding learnable parameters. \n",
    "\n",
    "    Uses sin and cos fcns of different frequencies to encode position info. \n",
    "\n",
    "    Args: \n",
    "        d_model (int): dimension of the embedding space \n",
    "        max_len (int, optional): max sequence length supported. Default is 5000 \n",
    "\n",
    "    Attriutes: \n",
    "        pe (Tensor): Fixed positional encoding matrix of space (max_len, 1, d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # the \"Attributes\" part documents the instance variables inside the __init__\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000): # creates init method \n",
    "        super(PositionalEncoding, self).__init__() # super() lets us avoid referring to the base class explicitly\n",
    "        # https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods\n",
    "        pe = torch.zeros(max_len, d_model) # create empty matrix of shape max_len X d_model to hold the positional encodings\n",
    "        # row: position i.e. 0, 1, 2, \n",
    "        # column: dim of the embedding \n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # vector of positions [0, 1, 2, 3, ..., 4999]\n",
    "        # unsqueeze reshapes vector from [max_len,] to [max_len, 1] to enable broadcasting \n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # division term for the sin and cos fcns \n",
    "        # torch.arange(0, d_model, 2).float(): starts at 0, ends at d_model, step size = 2 \n",
    "        # -ln(10000)\n",
    "        # torch.exp = exp \n",
    "        # this comes from \"Attention is all you need\" paper where sin and cos fcns of different frequencies are used where each dimension of the positional encoding corresponds to a sine\n",
    "        # PE at dim i = PE_(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        # PE_(pos, 2i+1) = cos(pos/10000^(2i/d_model)) \n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # at even indices: sin(position * frequency)\n",
    "    \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # at odd indices: cos(position * frequency)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        # pe.unsqueeze(0) == adds a batch dimension so the shape becomes: [1, max_len, d_model]\n",
    "        # .transpose(0,1) == swaps the first and second dimensions such that the new shape is [max_len, 1, d_model]\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        # saving pe tensor. Tensor which is not a parameter, but should be part of the module's state. Used for tensors that need to be on the same device as the module. \n",
    "        # it's a fixed tensor stored with the model and moved to the GPU/CPU automatically \n",
    "        # this is NOT updated during backprop \n",
    "\n",
    "    def forward(self, x): # during the forward pass, x is the input with shape [sequence length, batch_size, d_model]\n",
    "        return x + self.pe[:x.size(0), :] # add the pe for the len of the input, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35741f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USE OF POSITIONAL ENCODING\n",
    "positional_encoder = PositionalEncoding(d_model = 512)\n",
    "sample_x = torch.randn(100, 32, 512) # tensor filled with random numbers from a standard normal distribution of shape [100, 32, 512]\n",
    "sample_encode = positional_encoder(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ff6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -1.1524, -0.2506],\n",
      "         [-0.5461, -0.6302, -0.6347],\n",
      "         [-1.0841, -0.1287, -0.6811]],\n",
      "\n",
      "        [[-0.5518,  1.5398,  1.0036],\n",
      "         [-0.4424,  0.2087,  0.0160],\n",
      "         [ 1.2970, -0.4725,  0.3149]],\n",
      "\n",
      "        [[-0.9780,  0.6038, -1.7178],\n",
      "         [-0.3399, -0.2990,  1.8007],\n",
      "         [ 0.6786,  0.5225, -0.0246]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_x[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63089d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -0.1524, -0.2506],\n",
      "         [-0.5461,  0.3698, -0.6347],\n",
      "         [-1.0841,  0.8713, -0.6811]],\n",
      "\n",
      "        [[ 0.2896,  2.0801,  1.8254],\n",
      "         [ 0.3990,  0.7490,  0.8379],\n",
      "         [ 2.1385,  0.0678,  1.1368]],\n",
      "\n",
      "        [[-0.0687,  0.1877, -0.7814],\n",
      "         [ 0.5694, -0.7151,  2.7371],\n",
      "         [ 1.5879,  0.1063,  0.9118]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_encode[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f201e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module): # defines a model class \n",
    "    def __init__(self, feature_size=250, num_layers=1, dropout=0.1): # initialize model \n",
    "        # feature size: input embedding dimension \n",
    "        # num_layers: transformer layers to stack \n",
    "        # dropout: dropout rate for regularization \n",
    "\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer' # string label \n",
    "        self.src_mask = None # used for sequence masking - important in autoregressive tasks like time-series and language modeling\n",
    "        self.pos_encoder = PositionalEncoding(feature_size) # giving position awareness to the input embeddings before processed by the transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout) # one transformer encoder layer with 10 heads \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers) # stack multiple copies of the encoder layer \n",
    "        self.decoder = nn.Linear(feature_size, 1) # maps the feature size to 1 (in time series forecasting, this maps one number per position)\n",
    "        self.init_weights() # calls init_weights methods to initialize the weights \n",
    "\n",
    "    def init_weights(self): \n",
    "        initrange = 0.1 # define a small numer to set range for random initialization \n",
    "        self.decoder.bias.data.zero_() # bias is the additive constant inside Linear layers. This overwrites all biases to 0 so that only weights matter. Random biases could introduce unwanted drift right at the start so we initialize at 0\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange) # initialize weights between -0.1 and 0.1. Smaller weights help stabilize training early on \n",
    "\n",
    "    def forward(self,src): # the forward pass for the model \n",
    "\n",
    "        # because this is a causal task (like time series or language modeling), we must prevent tokens from seeing the future, so we implement this mask to avoid attention to future tokens\n",
    "        # the mask is a square matrix of size (seq_length, seq_length)\n",
    "\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) !=len(src): # check if a new source mask needs to be created (or a new one)\n",
    "            # if self.src_mask is None then no mask was created yet\n",
    "            #if self.src_mask.size(0) != len(src), the input seq len has changed since the previous iteration therefore we need one of a new size \n",
    "\n",
    "            device = src.device # get the device where the input tensor lives to ensure the mask is on the same device \n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device) \n",
    "            # generate causal mask of len(src) such that the token can attend only to itself and earlier tokens\n",
    "            self.src_mask = mask \n",
    "\n",
    "        src = self.pos_encoder(src) # apply positional encoding to the input embeddings \n",
    "        output = self.transformer_encoder(src, self.src_mask) # pass position-encoded input into the stacked transformer encoder layers \n",
    "        # this is where mlti-head self-attention happens \n",
    "        # each token attends to the previous ones bc of the mask \n",
    "\n",
    "        output = self.decoder(output)\n",
    "        # apply Linear layer to every position \n",
    "        # turn the feature_size vector to a scalar \n",
    "        # this is the model's final prediction at each time step or token \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz): \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1) # create upper triangular matrix of ones, then trnaspose flips it to make lower triangle of 1s \n",
    "        mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0)) # convert 1s/0s into attention scores. mask == 0 ==> future positions, mask == 1 ==> self and past \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda7300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare sequence data for training model\n",
    "\n",
    "def create_inout_sequences(input_data, tw): \n",
    "    # function: slices a time series dataset into overlapping i/o sequences for training \n",
    "    # input_data: 1D array (the time series)\n",
    "    # tw: time window (the sequence length)\n",
    "    # output: list of (input sequence, label sequence) pairs \n",
    "\n",
    "    inout_seq = [] # preallocate memory to hold the (input, label) pairs \n",
    "    L = len(input_data) # stores the length of the sequence \n",
    "    for i in range(L-tw): # loop through the sequence to get all sliding windows of length tw\n",
    "        # stop at L-tw to ensure input_data[i:i+tw] stays within the length of the input_data \n",
    "\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window], output_window * [0])\n",
    "        # input_data[i:i+tw]: gives a window of length tw \n",
    "        # [:-output_window]: removes the last output_window values - therefore we're making the last value(s) we want the model to predict \n",
    "        # output_window * [0]: appends 0s at the end for the same length removed\n",
    "\n",
    "        train_label = input_data[i:i+tw]\n",
    "        # this is the ground truth - the full unmasked windwo including the parts zeroed out \n",
    "\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "        # saves tuple of the masked input and the ground label \n",
    "\n",
    "    return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ff5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates and prepares synthetic time series data for the transformer, including normalizaton and splitting into training and tests sets \n",
    "def get_data(): \n",
    "\n",
    "    # generate synthetic data of a continuous signal \n",
    "    time = np.arange(0, 400, 0.1) # len = 4000 \n",
    "    amplitude = np.sin(time) + np.sin(time * 0.05) + np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
    "    # np.sin(time): main wave \n",
    "    # np.sin(time*0.05): low-frequency drift\n",
    "    # np.sin(time*0.12): modulated noise \n",
    "\n",
    "\n",
    "    # scaling the data \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    # reshape(-1, 1): turns a 1D array into a 2D shape expected by MinMaxScaler, then .reshape(-1) flattens it again \n",
    "\n",
    "    # data splitting \n",
    "    samples = 2800 # 70% of 4000\n",
    "    train_data = amplitude[:samples]\n",
    "    test_data = amplitude[samples:]\n",
    "\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2d6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, batch_size): # extracts a mini-batch of input/label pairs from the training or test set (the source), starting at position i, and prepares the right shape for the model input\n",
    "    seq_len = min(batch_size, len(source) - 1 - i) # calculates how many items to include in the current batch - if you're near the end of the dataset, you may not have enough for a full batch so you'll take whatever is left\n",
    "    data = source[i:i+seq_len] # slices the dataset from i to i+seq_len\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a4a2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data): \n",
    "    # define the train function, takes in train_data, process it in batches to train the model for one epoch\n",
    "    model.train() # sets the mode to training mode \n",
    "    # this enables layers like Dropout and BatchNorm to behave correctly - as opposed to how they do during .eval() mode\n",
    "\n",
    "    total_loss = 0.0 # initialize loss counter \n",
    "    start_time = time.time() # starts timer at start of training  - used to calculate how long each batch takes\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        # enumerate gives: batch size index, the start index of the train_data\n",
    "\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        # extracts a batch of inputs starting at index i \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # clear old gradients from previous step \n",
    "\n",
    "        output = model(data)\n",
    "        # feeds input batch into the model to store the predictions\n",
    "\n",
    "        if calculate_loss_over_all_values: # uses entire sequence to calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "        else: # uses only the previous output window of timesteps to calculate loss\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "\n",
    "        loss.backward() # compute gradients of loss wrt the learnable parameters\n",
    "        # backward pass \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # clip gradients if norm exceeds 0.5 to prevent exploding gradients \n",
    "        optimizer.step() # apply gradients to update model weights \n",
    "        # this is where learning occurs \n",
    "\n",
    "        total_loss += loss.item() # add current loss to cummulative loss\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval \n",
    "            elapsed = time.time() - start_time \n",
    "            print ('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                    ' lr {:02.6f} | {:5.2f} ms | loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            # ppl = perplexity\n",
    "            total_loss = 0 \n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc04b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source, epoch): \n",
    "    # evaluates model on the dataset, data_source \n",
    "\n",
    "    eval_model.eval() # sets model to evaluatio model \n",
    "    total_loss = 0.0 # accumulated loss across test examples \n",
    "    test_result = torch.Tensor(0) # stores model outputs for plotting \n",
    "    truth = torch.Tensor(0) # ground truth values\n",
    "\n",
    "    #  torch.Tensor(0): creates empty 1D tensors \n",
    "\n",
    "    with torch.no_grad(): # opens the no-gradient context \n",
    "        # during evaluation, we don't need gradients \n",
    "\n",
    "        for i in range(0, len(data_source) - 1): # iterate over every sample of data_source except the last\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else: \n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "    \n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color = \"red\") # model predictions \n",
    "    pyplot.plot(truth[:500], color = \"blue\") # true values \n",
    "    pyplot.plot(test_result - truth, color = \"green\") # error\n",
    "    pyplot.grid(True, which = 'both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5df8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(eval_model, data_source, steps): \n",
    "    #  enables trained transformer to predict future time steps beyond the training window \n",
    "    eval_model.eval() # puts model in evaluation mode \n",
    "    total_loss = 0.0\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    _, data = get_batch(data_source, 0, 1) # obtain single sample from dataset to use as the initial input sequence \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps, 1): \n",
    "            input = torch.clone(data[-input_window:]) # create copy of last input_window timesteps from data\n",
    "            input[-output_window:] = 0 # zero out the last output_window values in the input. Simulates missing future steps for the model to predict \n",
    "            output = eval_model(data[-input_window:]) \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "        data = data.cpu().view(-1)\n",
    "\n",
    "        pyplot.plot(data, color = 'red') # full series (original and predicted)\n",
    "        pyplot.plot(data[:input_window], color = 'blue') # initial known input \n",
    "        pyplot.grid(True, which = 'both') \n",
    "        pyplot.axhline(y=0, color='k')\n",
    "        pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "        pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60eca47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source): \n",
    "    # defines function to evaluate the model's loss on the data_source dataset - this is called after each epoch for the validation set \n",
    "    eval_model.eval() # set model to evaluation mode\n",
    "    total_loss = 0.0 # initialize total loss\n",
    "    eval_batch_size = 1000 # set large batch size since we're not doing backprop \n",
    "    with torch.no_grad(): # being no-gradient context \n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "            else: \n",
    "                total_loss += len(data[0]) * criterion(output[-output_window:], targets[-output_window:]).cpu().item()\n",
    "            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f45371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_98572/1047780709.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_2634bauad6/croot/libtorch_1744642078920/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:536: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches |  lr 0.005000 | 335.04 ms | loss 5.37208 | ppl   215.31\n",
      "| epoch   1 |   106/  269 batches |  lr 0.005000 | 333.80 ms | loss 0.18379 | ppl     1.20\n",
      "| epoch   1 |   159/  269 batches |  lr 0.005000 | 469.43 ms | loss 0.14173 | ppl     1.15\n",
      "| epoch   1 |   212/  269 batches |  lr 0.005000 | 429.62 ms | loss 0.13458 | ppl     1.14\n",
      "| epoch   1 |   265/  269 batches |  lr 0.005000 | 296.27 ms | loss 0.12767 | ppl     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1e+02s | valid loss 0.30951 | valid ppl     1.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches |  lr 0.004802 | 305.65 ms | loss 0.12207 | ppl     1.13\n",
      "| epoch   2 |   106/  269 batches |  lr 0.004802 | 287.83 ms | loss 0.11788 | ppl     1.13\n",
      "| epoch   2 |   159/  269 batches |  lr 0.004802 | 252.48 ms | loss 0.06676 | ppl     1.07\n",
      "| epoch   2 |   212/  269 batches |  lr 0.004802 | 361.05 ms | loss 0.07154 | ppl     1.07\n",
      "| epoch   2 |   265/  269 batches |  lr 0.004802 | 361.14 ms | loss 0.07703 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 8.8e+01s | valid loss 0.50593 | valid ppl     1.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches |  lr 0.004706 | 428.53 ms | loss 0.08824 | ppl     1.09\n",
      "| epoch   3 |   106/  269 batches |  lr 0.004706 | 448.75 ms | loss 0.05057 | ppl     1.05\n",
      "| epoch   3 |   159/  269 batches |  lr 0.004706 | 362.49 ms | loss 0.02811 | ppl     1.03\n",
      "| epoch   3 |   212/  269 batches |  lr 0.004706 | 311.24 ms | loss 0.01528 | ppl     1.02\n",
      "| epoch   3 |   265/  269 batches |  lr 0.004706 | 367.50 ms | loss 0.03391 | ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1.1e+02s | valid loss 0.16522 | valid ppl     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches |  lr 0.004612 | 442.49 ms | loss 0.01574 | ppl     1.02\n",
      "| epoch   4 |   106/  269 batches |  lr 0.004612 | 354.96 ms | loss 0.01773 | ppl     1.02\n",
      "| epoch   4 |   159/  269 batches |  lr 0.004612 | 294.03 ms | loss 0.01751 | ppl     1.02\n",
      "| epoch   4 |   212/  269 batches |  lr 0.004612 | 359.12 ms | loss 0.01340 | ppl     1.01\n",
      "| epoch   4 |   265/  269 batches |  lr 0.004612 | 281.00 ms | loss 0.02586 | ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 9.6e+01s | valid loss 0.39864 | valid ppl     1.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches |  lr 0.004520 | 300.90 ms | loss 0.03858 | ppl     1.04\n",
      "| epoch   5 |   106/  269 batches |  lr 0.004520 | 346.42 ms | loss 0.03042 | ppl     1.03\n",
      "| epoch   5 |   159/  269 batches |  lr 0.004520 | 349.95 ms | loss 0.01742 | ppl     1.02\n",
      "| epoch   5 |   212/  269 batches |  lr 0.004520 | 302.20 ms | loss 0.01165 | ppl     1.01\n",
      "| epoch   5 |   265/  269 batches |  lr 0.004520 | 384.60 ms | loss 0.01315 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 9.3e+01s | valid loss 0.37452 | valid ppl     1.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches |  lr 0.004429 | 422.88 ms | loss 0.02812 | ppl     1.03\n",
      "| epoch   6 |   106/  269 batches |  lr 0.004429 | 335.92 ms | loss 0.01637 | ppl     1.02\n",
      "| epoch   6 |   159/  269 batches |  lr 0.004429 | 351.43 ms | loss 0.01444 | ppl     1.01\n",
      "| epoch   6 |   212/  269 batches |  lr 0.004429 | 376.95 ms | loss 0.01404 | ppl     1.01\n",
      "| epoch   6 |   265/  269 batches |  lr 0.004429 | 335.49 ms | loss 0.01270 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 1e+02s | valid loss 0.40679 | valid ppl     1.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches |  lr 0.004341 | 436.35 ms | loss 0.03413 | ppl     1.03\n",
      "| epoch   7 |   106/  269 batches |  lr 0.004341 | 275.50 ms | loss 0.01628 | ppl     1.02\n",
      "| epoch   7 |   159/  269 batches |  lr 0.004341 | 257.99 ms | loss 0.01982 | ppl     1.02\n",
      "| epoch   7 |   212/  269 batches |  lr 0.004341 | 235.53 ms | loss 0.01302 | ppl     1.01\n",
      "| epoch   7 |   265/  269 batches |  lr 0.004341 | 237.35 ms | loss 0.01463 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 8e+01s | valid loss 0.31268 | valid ppl     1.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    53/  269 batches |  lr 0.004254 | 309.09 ms | loss 0.02591 | ppl     1.03\n",
      "| epoch   8 |   106/  269 batches |  lr 0.004254 | 244.18 ms | loss 0.01486 | ppl     1.01\n",
      "| epoch   8 |   159/  269 batches |  lr 0.004254 | 248.34 ms | loss 0.01397 | ppl     1.01\n",
      "| epoch   8 |   212/  269 batches |  lr 0.004254 | 238.59 ms | loss 0.01227 | ppl     1.01\n",
      "| epoch   8 |   265/  269 batches |  lr 0.004254 | 305.42 ms | loss 0.01046 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 7.5e+01s | valid loss 0.25707 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    53/  269 batches |  lr 0.004169 | 330.59 ms | loss 0.02574 | ppl     1.03\n",
      "| epoch   9 |   106/  269 batches |  lr 0.004169 | 366.46 ms | loss 0.01155 | ppl     1.01\n",
      "| epoch   9 |   159/  269 batches |  lr 0.004169 | 318.28 ms | loss 0.01356 | ppl     1.01\n",
      "| epoch   9 |   212/  269 batches |  lr 0.004169 | 289.82 ms | loss 0.00949 | ppl     1.01\n",
      "| epoch   9 |   265/  269 batches |  lr 0.004169 | 283.23 ms | loss 0.01262 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 8.8e+01s | valid loss 0.26454 | valid ppl     1.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    53/  269 batches |  lr 0.004085 | 315.66 ms | loss 0.02624 | ppl     1.03\n",
      "| epoch  10 |   106/  269 batches |  lr 0.004085 | 344.53 ms | loss 0.01269 | ppl     1.01\n",
      "| epoch  10 |   159/  269 batches |  lr 0.004085 | 312.75 ms | loss 0.01726 | ppl     1.02\n",
      "| epoch  10 |   212/  269 batches |  lr 0.004085 | 276.28 ms | loss 0.00970 | ppl     1.01\n",
      "| epoch  10 |   265/  269 batches |  lr 0.004085 | 388.95 ms | loss 0.01076 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 1.4e+02s | valid loss 0.24634 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    53/  269 batches |  lr 0.004004 | 433.66 ms | loss 0.02446 | ppl     1.02\n",
      "| epoch  11 |   106/  269 batches |  lr 0.004004 | 296.42 ms | loss 0.01220 | ppl     1.01\n",
      "| epoch  11 |   159/  269 batches |  lr 0.004004 | 314.47 ms | loss 0.01473 | ppl     1.01\n",
      "| epoch  11 |   212/  269 batches |  lr 0.004004 | 338.75 ms | loss 0.00873 | ppl     1.01\n",
      "| epoch  11 |   265/  269 batches |  lr 0.004004 | 311.49 ms | loss 0.01364 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 9.4e+01s | valid loss 0.24636 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    53/  269 batches |  lr 0.003924 | 362.54 ms | loss 0.02299 | ppl     1.02\n",
      "| epoch  12 |   106/  269 batches |  lr 0.003924 | 406.91 ms | loss 0.01063 | ppl     1.01\n",
      "| epoch  12 |   159/  269 batches |  lr 0.003924 | 362.08 ms | loss 0.01340 | ppl     1.01\n",
      "| epoch  12 |   212/  269 batches |  lr 0.003924 | 316.59 ms | loss 0.00986 | ppl     1.01\n",
      "| epoch  12 |   265/  269 batches |  lr 0.003924 | 445.20 ms | loss 0.01049 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 1e+02s | valid loss 0.27608 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    53/  269 batches |  lr 0.003845 | 387.04 ms | loss 0.03197 | ppl     1.03\n",
      "| epoch  13 |   106/  269 batches |  lr 0.003845 | 397.23 ms | loss 0.01493 | ppl     1.02\n",
      "| epoch  13 |   159/  269 batches |  lr 0.003845 | 406.19 ms | loss 0.01082 | ppl     1.01\n",
      "| epoch  13 |   212/  269 batches |  lr 0.003845 | 375.68 ms | loss 0.01113 | ppl     1.01\n",
      "| epoch  13 |   265/  269 batches |  lr 0.003845 | 328.36 ms | loss 0.01011 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 1.1e+02s | valid loss 0.27226 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    53/  269 batches |  lr 0.003768 | 381.97 ms | loss 0.03163 | ppl     1.03\n",
      "| epoch  14 |   106/  269 batches |  lr 0.003768 | 365.03 ms | loss 0.01101 | ppl     1.01\n",
      "| epoch  14 |   159/  269 batches |  lr 0.003768 | 334.79 ms | loss 0.01327 | ppl     1.01\n",
      "| epoch  14 |   212/  269 batches |  lr 0.003768 | 429.72 ms | loss 0.00897 | ppl     1.01\n",
      "| epoch  14 |   265/  269 batches |  lr 0.003768 | 327.35 ms | loss 0.01142 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 1e+02s | valid loss 0.26906 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    53/  269 batches |  lr 0.003693 | 383.99 ms | loss 0.03341 | ppl     1.03\n",
      "| epoch  15 |   106/  269 batches |  lr 0.003693 | 360.51 ms | loss 0.02006 | ppl     1.02\n",
      "| epoch  15 |   159/  269 batches |  lr 0.003693 | 322.22 ms | loss 0.01472 | ppl     1.01\n",
      "| epoch  15 |   212/  269 batches |  lr 0.003693 | 308.74 ms | loss 0.00884 | ppl     1.01\n",
      "| epoch  15 |   265/  269 batches |  lr 0.003693 | 419.56 ms | loss 0.00894 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 9.9e+01s | valid loss 0.25760 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    53/  269 batches |  lr 0.003619 | 372.42 ms | loss 0.03269 | ppl     1.03\n",
      "| epoch  16 |   106/  269 batches |  lr 0.003619 | 323.84 ms | loss 0.02440 | ppl     1.02\n",
      "| epoch  16 |   159/  269 batches |  lr 0.003619 | 375.91 ms | loss 0.01036 | ppl     1.01\n",
      "| epoch  16 |   212/  269 batches |  lr 0.003619 | 373.05 ms | loss 0.00864 | ppl     1.01\n",
      "| epoch  16 |   265/  269 batches |  lr 0.003619 | 333.66 ms | loss 0.00754 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 1e+02s | valid loss 0.27431 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    53/  269 batches |  lr 0.003547 | 436.69 ms | loss 0.03364 | ppl     1.03\n",
      "| epoch  17 |   106/  269 batches |  lr 0.003547 | 350.14 ms | loss 0.02036 | ppl     1.02\n",
      "| epoch  17 |   159/  269 batches |  lr 0.003547 | 328.01 ms | loss 0.01300 | ppl     1.01\n",
      "| epoch  17 |   212/  269 batches |  lr 0.003547 | 339.71 ms | loss 0.00801 | ppl     1.01\n",
      "| epoch  17 |   265/  269 batches |  lr 0.003547 | 256.58 ms | loss 0.00888 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 9.4e+01s | valid loss 0.25336 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    53/  269 batches |  lr 0.003476 | 290.75 ms | loss 0.03124 | ppl     1.03\n",
      "| epoch  18 |   106/  269 batches |  lr 0.003476 | 335.62 ms | loss 0.01744 | ppl     1.02\n",
      "| epoch  18 |   159/  269 batches |  lr 0.003476 | 334.83 ms | loss 0.01248 | ppl     1.01\n",
      "| epoch  18 |   212/  269 batches |  lr 0.003476 | 276.80 ms | loss 0.00794 | ppl     1.01\n",
      "| epoch  18 |   265/  269 batches |  lr 0.003476 | 345.57 ms | loss 0.00720 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 8.9e+01s | valid loss 0.26239 | valid ppl     1.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    53/  269 batches |  lr 0.003406 | 368.61 ms | loss 0.02647 | ppl     1.03\n",
      "| epoch  19 |   106/  269 batches |  lr 0.003406 | 295.73 ms | loss 0.01441 | ppl     1.01\n",
      "| epoch  19 |   159/  269 batches |  lr 0.003406 | 272.43 ms | loss 0.01153 | ppl     1.01\n",
      "| epoch  19 |   212/  269 batches |  lr 0.003406 | 335.13 ms | loss 0.00833 | ppl     1.01\n",
      "| epoch  19 |   265/  269 batches |  lr 0.003406 | 402.58 ms | loss 0.00899 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 9.4e+01s | valid loss 0.27352 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    53/  269 batches |  lr 0.003338 | 335.74 ms | loss 0.02849 | ppl     1.03\n",
      "| epoch  20 |   106/  269 batches |  lr 0.003338 | 348.67 ms | loss 0.01870 | ppl     1.02\n",
      "| epoch  20 |   159/  269 batches |  lr 0.003338 | 275.07 ms | loss 0.01121 | ppl     1.01\n",
      "| epoch  20 |   212/  269 batches |  lr 0.003338 | 296.30 ms | loss 0.00807 | ppl     1.01\n",
      "| epoch  20 |   265/  269 batches |  lr 0.003338 | 290.72 ms | loss 0.00796 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 1.2e+02s | valid loss 0.25712 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    53/  269 batches |  lr 0.003271 | 254.90 ms | loss 0.02528 | ppl     1.03\n",
      "| epoch  21 |   106/  269 batches |  lr 0.003271 | 295.13 ms | loss 0.01613 | ppl     1.02\n",
      "| epoch  21 |   159/  269 batches |  lr 0.003271 | 279.43 ms | loss 0.01089 | ppl     1.01\n",
      "| epoch  21 |   212/  269 batches |  lr 0.003271 | 276.26 ms | loss 0.00756 | ppl     1.01\n",
      "| epoch  21 |   265/  269 batches |  lr 0.003271 | 325.63 ms | loss 0.00772 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 8.1e+01s | valid loss 0.24795 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    53/  269 batches |  lr 0.003206 | 271.55 ms | loss 0.02199 | ppl     1.02\n",
      "| epoch  22 |   106/  269 batches |  lr 0.003206 | 255.37 ms | loss 0.00982 | ppl     1.01\n",
      "| epoch  22 |   159/  269 batches |  lr 0.003206 | 258.06 ms | loss 0.01106 | ppl     1.01\n",
      "| epoch  22 |   212/  269 batches |  lr 0.003206 | 250.39 ms | loss 0.00754 | ppl     1.01\n",
      "| epoch  22 |   265/  269 batches |  lr 0.003206 | 300.86 ms | loss 0.00926 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 7.4e+01s | valid loss 0.24752 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    53/  269 batches |  lr 0.003142 | 255.95 ms | loss 0.02557 | ppl     1.03\n",
      "| epoch  23 |   106/  269 batches |  lr 0.003142 | 268.18 ms | loss 0.01270 | ppl     1.01\n",
      "| epoch  23 |   159/  269 batches |  lr 0.003142 | 257.51 ms | loss 0.01064 | ppl     1.01\n",
      "| epoch  23 |   212/  269 batches |  lr 0.003142 | 295.55 ms | loss 0.00785 | ppl     1.01\n",
      "| epoch  23 |   265/  269 batches |  lr 0.003142 | 3456.10 ms | loss 0.00774 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 2.4e+02s | valid loss 0.25236 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    53/  269 batches |  lr 0.003079 | 459.68 ms | loss 0.02324 | ppl     1.02\n",
      "| epoch  24 |   106/  269 batches |  lr 0.003079 | 354.01 ms | loss 0.01053 | ppl     1.01\n",
      "| epoch  24 |   159/  269 batches |  lr 0.003079 | 389.05 ms | loss 0.00998 | ppl     1.01\n",
      "| epoch  24 |   212/  269 batches |  lr 0.003079 | 374.55 ms | loss 0.00740 | ppl     1.01\n",
      "| epoch  24 |   265/  269 batches |  lr 0.003079 | 326.97 ms | loss 0.01058 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 1.1e+02s | valid loss 0.25071 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    53/  269 batches |  lr 0.003017 | 383.58 ms | loss 0.02644 | ppl     1.03\n",
      "| epoch  25 |   106/  269 batches |  lr 0.003017 | 444.89 ms | loss 0.01195 | ppl     1.01\n",
      "| epoch  25 |   159/  269 batches |  lr 0.003017 | 535.76 ms | loss 0.01032 | ppl     1.01\n",
      "| epoch  25 |   212/  269 batches |  lr 0.003017 | 4797.82 ms | loss 0.00794 | ppl     1.01\n",
      "| epoch  25 |   265/  269 batches |  lr 0.003017 | 15817.87 ms | loss 0.00970 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 1.2e+03s | valid loss 0.26877 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    53/  269 batches |  lr 0.002957 | 7899.79 ms | loss 0.02626 | ppl     1.03\n",
      "| epoch  26 |   106/  269 batches |  lr 0.002957 | 1668.21 ms | loss 0.01112 | ppl     1.01\n",
      "| epoch  26 |   159/  269 batches |  lr 0.002957 | 787.82 ms | loss 0.01041 | ppl     1.01\n",
      "| epoch  26 |   212/  269 batches |  lr 0.002957 | 1881.30 ms | loss 0.00660 | ppl     1.01\n",
      "| epoch  26 |   265/  269 batches |  lr 0.002957 | 1387.38 ms | loss 0.00756 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 7.3e+02s | valid loss 0.27586 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    53/  269 batches |  lr 0.002898 | 2442.05 ms | loss 0.02719 | ppl     1.03\n",
      "| epoch  27 |   106/  269 batches |  lr 0.002898 | 3068.31 ms | loss 0.01121 | ppl     1.01\n",
      "| epoch  27 |   159/  269 batches |  lr 0.002898 | 1134.99 ms | loss 0.00975 | ppl     1.01\n",
      "| epoch  27 |   212/  269 batches |  lr 0.002898 | 3057.04 ms | loss 0.00721 | ppl     1.01\n",
      "| epoch  27 |   265/  269 batches |  lr 0.002898 | 1403.33 ms | loss 0.00906 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 5.9e+02s | valid loss 0.25449 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    53/  269 batches |  lr 0.002840 | 4884.59 ms | loss 0.02474 | ppl     1.03\n",
      "| epoch  28 |   106/  269 batches |  lr 0.002840 | 5406.17 ms | loss 0.01038 | ppl     1.01\n",
      "| epoch  28 |   159/  269 batches |  lr 0.002840 | 1688.77 ms | loss 0.01074 | ppl     1.01\n",
      "| epoch  28 |   212/  269 batches |  lr 0.002840 | 912.87 ms | loss 0.00789 | ppl     1.01\n",
      "| epoch  28 |   265/  269 batches |  lr 0.002840 | 22148.75 ms | loss 0.00751 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 1.9e+03s | valid loss 0.26807 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    53/  269 batches |  lr 0.002783 | 2012.23 ms | loss 0.02482 | ppl     1.03\n",
      "| epoch  29 |   106/  269 batches |  lr 0.002783 | 13060.56 ms | loss 0.00852 | ppl     1.01\n",
      "| epoch  29 |   159/  269 batches |  lr 0.002783 | 1314.90 ms | loss 0.00997 | ppl     1.01\n",
      "| epoch  29 |   212/  269 batches |  lr 0.002783 | 17999.33 ms | loss 0.00749 | ppl     1.01\n",
      "| epoch  29 |   265/  269 batches |  lr 0.002783 | 11831.23 ms | loss 0.00713 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 2.5e+03s | valid loss 0.27772 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    53/  269 batches |  lr 0.002727 | 17741.35 ms | loss 0.02605 | ppl     1.03\n",
      "| epoch  30 |   106/  269 batches |  lr 0.002727 | 1940.33 ms | loss 0.00890 | ppl     1.01\n",
      "| epoch  30 |   159/  269 batches |  lr 0.002727 | 6815.32 ms | loss 0.01060 | ppl     1.01\n",
      "| epoch  30 |   212/  269 batches |  lr 0.002727 | 21514.36 ms | loss 0.00713 | ppl     1.01\n",
      "| epoch  30 |   265/  269 batches |  lr 0.002727 | 17744.84 ms | loss 0.00756 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 5.5e+03s | valid loss 0.27402 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    53/  269 batches |  lr 0.002673 | 17875.95 ms | loss 0.02620 | ppl     1.03\n",
      "| epoch  31 |   106/  269 batches |  lr 0.002673 | 16607.35 ms | loss 0.00846 | ppl     1.01\n",
      "| epoch  31 |   159/  269 batches |  lr 0.002673 | 31217.90 ms | loss 0.01000 | ppl     1.01\n",
      "| epoch  31 |   212/  269 batches |  lr 0.002673 | 34878.98 ms | loss 0.00741 | ppl     1.01\n",
      "| epoch  31 |   265/  269 batches |  lr 0.002673 | 1858.48 ms | loss 0.00814 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 5.7e+03s | valid loss 0.27771 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    53/  269 batches |  lr 0.002619 | 17936.36 ms | loss 0.02596 | ppl     1.03\n",
      "| epoch  32 |   106/  269 batches |  lr 0.002619 | 2613.35 ms | loss 0.00857 | ppl     1.01\n",
      "| epoch  32 |   159/  269 batches |  lr 0.002619 | 17784.06 ms | loss 0.01058 | ppl     1.01\n",
      "| epoch  32 |   212/  269 batches |  lr 0.002619 | 17973.49 ms | loss 0.00706 | ppl     1.01\n",
      "| epoch  32 |   265/  269 batches |  lr 0.002619 | 30199.70 ms | loss 0.00831 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 4.6e+03s | valid loss 0.27595 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    53/  269 batches |  lr 0.002567 | 24611.33 ms | loss 0.02460 | ppl     1.02\n",
      "| epoch  33 |   106/  269 batches |  lr 0.002567 | 17852.79 ms | loss 0.00830 | ppl     1.01\n",
      "| epoch  33 |   159/  269 batches |  lr 0.002567 | 21592.71 ms | loss 0.00980 | ppl     1.01\n",
      "| epoch  33 |   212/  269 batches |  lr 0.002567 | 17845.54 ms | loss 0.00692 | ppl     1.01\n",
      "| epoch  33 |   265/  269 batches |  lr 0.002567 | 17916.23 ms | loss 0.00714 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 5.3e+03s | valid loss 0.26636 | valid ppl     1.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    53/  269 batches |  lr 0.002516 | 33273.78 ms | loss 0.02474 | ppl     1.03\n",
      "| epoch  34 |   106/  269 batches |  lr 0.002516 | 778.52 ms | loss 0.00902 | ppl     1.01\n",
      "| epoch  34 |   159/  269 batches |  lr 0.002516 | 19472.23 ms | loss 0.01103 | ppl     1.01\n",
      "| epoch  34 |   212/  269 batches |  lr 0.002516 | 17975.84 ms | loss 0.00888 | ppl     1.01\n",
      "| epoch  34 |   265/  269 batches |  lr 0.002516 | 30850.61 ms | loss 0.00879 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 5.4e+03s | valid loss 0.25250 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    53/  269 batches |  lr 0.002465 | 34755.95 ms | loss 0.02646 | ppl     1.03\n",
      "| epoch  35 |   106/  269 batches |  lr 0.002465 | 34076.32 ms | loss 0.01058 | ppl     1.01\n",
      "| epoch  35 |   159/  269 batches |  lr 0.002465 | 17961.12 ms | loss 0.01005 | ppl     1.01\n",
      "| epoch  35 |   212/  269 batches |  lr 0.002465 | 17829.36 ms | loss 0.00806 | ppl     1.01\n",
      "| epoch  35 |   265/  269 batches |  lr 0.002465 | 16370.21 ms | loss 0.00807 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 6.4e+03s | valid loss 0.26000 | valid ppl     1.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    53/  269 batches |  lr 0.002416 | 35024.13 ms | loss 0.02663 | ppl     1.03\n",
      "| epoch  36 |   106/  269 batches |  lr 0.002416 | 17892.99 ms | loss 0.00987 | ppl     1.01\n",
      "| epoch  36 |   159/  269 batches |  lr 0.002416 | 15945.67 ms | loss 0.01160 | ppl     1.01\n",
      "| epoch  36 |   212/  269 batches |  lr 0.002416 | 18049.16 ms | loss 0.00925 | ppl     1.01\n",
      "| epoch  36 |   265/  269 batches |  lr 0.002416 | 34869.81 ms | loss 0.00816 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 6.5e+03s | valid loss 0.27459 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    53/  269 batches |  lr 0.002368 | 15529.20 ms | loss 0.02434 | ppl     1.02\n",
      "| epoch  37 |   106/  269 batches |  lr 0.002368 | 8515.26 ms | loss 0.00920 | ppl     1.01\n",
      "| epoch  37 |   159/  269 batches |  lr 0.002368 | 14100.97 ms | loss 0.01015 | ppl     1.01\n",
      "| epoch  37 |   212/  269 batches |  lr 0.002368 | 34803.83 ms | loss 0.00900 | ppl     1.01\n",
      "| epoch  37 |   265/  269 batches |  lr 0.002368 | 11602.35 ms | loss 0.00788 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 4.9e+03s | valid loss 0.26198 | valid ppl     1.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    53/  269 batches |  lr 0.002320 | 36865.08 ms | loss 0.02426 | ppl     1.02\n",
      "| epoch  38 |   106/  269 batches |  lr 0.002320 | 17857.81 ms | loss 0.00892 | ppl     1.01\n",
      "| epoch  38 |   159/  269 batches |  lr 0.002320 | 6001.46 ms | loss 0.01062 | ppl     1.01\n",
      "| epoch  38 |   212/  269 batches |  lr 0.002320 | 17935.25 ms | loss 0.00988 | ppl     1.01\n",
      "| epoch  38 |   265/  269 batches |  lr 0.002320 | 15468.53 ms | loss 0.00758 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 5e+03s | valid loss 0.29243 | valid ppl     1.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    53/  269 batches |  lr 0.002274 | 766.23 ms | loss 0.02451 | ppl     1.02\n",
      "| epoch  39 |   106/  269 batches |  lr 0.002274 | 1562.07 ms | loss 0.00838 | ppl     1.01\n",
      "| epoch  39 |   159/  269 batches |  lr 0.002274 | 781.96 ms | loss 0.01153 | ppl     1.01\n",
      "| epoch  39 |   212/  269 batches |  lr 0.002274 | 821.58 ms | loss 0.01078 | ppl     1.01\n",
      "| epoch  39 |   265/  269 batches |  lr 0.002274 | 864.65 ms | loss 0.00793 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 2.6e+02s | valid loss 0.30775 | valid ppl     1.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    53/  269 batches |  lr 0.002229 | 512.33 ms | loss 0.02644 | ppl     1.03\n",
      "| epoch  40 |   106/  269 batches |  lr 0.002229 | 490.58 ms | loss 0.00802 | ppl     1.01\n",
      "| epoch  40 |   159/  269 batches |  lr 0.002229 | 525.91 ms | loss 0.01372 | ppl     1.01\n",
      "| epoch  40 |   212/  269 batches |  lr 0.002229 | 1554.98 ms | loss 0.01057 | ppl     1.01\n",
      "| epoch  40 |   265/  269 batches |  lr 0.002229 | 472.34 ms | loss 0.00781 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 2.6e+02s | valid loss 0.29787 | valid ppl     1.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    53/  269 batches |  lr 0.002184 | 370.54 ms | loss 0.02652 | ppl     1.03\n",
      "| epoch  41 |   106/  269 batches |  lr 0.002184 | 405.96 ms | loss 0.00851 | ppl     1.01\n",
      "| epoch  41 |   159/  269 batches |  lr 0.002184 | 377.99 ms | loss 0.01102 | ppl     1.01\n",
      "| epoch  41 |   212/  269 batches |  lr 0.002184 | 372.39 ms | loss 0.01065 | ppl     1.01\n",
      "| epoch  41 |   265/  269 batches |  lr 0.002184 | 408.46 ms | loss 0.00778 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 1.1e+02s | valid loss 0.28114 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    53/  269 batches |  lr 0.002140 | 425.33 ms | loss 0.03132 | ppl     1.03\n",
      "| epoch  42 |   106/  269 batches |  lr 0.002140 | 456.01 ms | loss 0.00976 | ppl     1.01\n",
      "| epoch  42 |   159/  269 batches |  lr 0.002140 | 3770.24 ms | loss 0.01353 | ppl     1.01\n",
      "| epoch  42 |   212/  269 batches |  lr 0.002140 | 2746.05 ms | loss 0.01219 | ppl     1.01\n",
      "| epoch  42 |   265/  269 batches |  lr 0.002140 | 778.72 ms | loss 0.00830 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 4.4e+02s | valid loss 0.27704 | valid ppl     1.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    53/  269 batches |  lr 0.002097 | 5075.45 ms | loss 0.02980 | ppl     1.03\n",
      "| epoch  43 |   106/  269 batches |  lr 0.002097 | 2930.12 ms | loss 0.01326 | ppl     1.01\n",
      "| epoch  43 |   159/  269 batches |  lr 0.002097 | 830.80 ms | loss 0.01526 | ppl     1.02\n",
      "| epoch  43 |   212/  269 batches |  lr 0.002097 | 609.02 ms | loss 0.01193 | ppl     1.01\n",
      "| epoch  43 |   265/  269 batches |  lr 0.002097 | 477.28 ms | loss 0.01007 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 5.3e+02s | valid loss 0.24830 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    53/  269 batches |  lr 0.002055 | 364.30 ms | loss 0.03410 | ppl     1.03\n",
      "| epoch  44 |   106/  269 batches |  lr 0.002055 | 336.30 ms | loss 0.02075 | ppl     1.02\n",
      "| epoch  44 |   159/  269 batches |  lr 0.002055 | 398.11 ms | loss 0.01822 | ppl     1.02\n",
      "| epoch  44 |   212/  269 batches |  lr 0.002055 | 373.59 ms | loss 0.01162 | ppl     1.01\n",
      "| epoch  44 |   265/  269 batches |  lr 0.002055 | 381.68 ms | loss 0.00985 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 1e+02s | valid loss 0.21784 | valid ppl     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    53/  269 batches |  lr 0.002014 | 436.14 ms | loss 0.03479 | ppl     1.04\n",
      "| epoch  45 |   106/  269 batches |  lr 0.002014 | 347.60 ms | loss 0.03836 | ppl     1.04\n",
      "| epoch  45 |   159/  269 batches |  lr 0.002014 | 816.76 ms | loss 0.02534 | ppl     1.03\n",
      "| epoch  45 |   212/  269 batches |  lr 0.002014 | 3619.46 ms | loss 0.01151 | ppl     1.01\n",
      "| epoch  45 |   265/  269 batches |  lr 0.002014 | 1698.13 ms | loss 0.01208 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 4.8e+02s | valid loss 0.16715 | valid ppl     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    53/  269 batches |  lr 0.001974 | 439.95 ms | loss 0.02734 | ppl     1.03\n",
      "| epoch  46 |   106/  269 batches |  lr 0.001974 | 359.69 ms | loss 0.03712 | ppl     1.04\n",
      "| epoch  46 |   159/  269 batches |  lr 0.001974 | 377.08 ms | loss 0.03276 | ppl     1.03\n",
      "| epoch  46 |   212/  269 batches |  lr 0.001974 | 400.95 ms | loss 0.01277 | ppl     1.01\n",
      "| epoch  46 |   265/  269 batches |  lr 0.001974 | 357.18 ms | loss 0.01537 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 1.1e+02s | valid loss 0.16493 | valid ppl     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    53/  269 batches |  lr 0.001935 | 360.09 ms | loss 0.02632 | ppl     1.03\n",
      "| epoch  47 |   106/  269 batches |  lr 0.001935 | 378.31 ms | loss 0.02502 | ppl     1.03\n",
      "| epoch  47 |   159/  269 batches |  lr 0.001935 | 328.93 ms | loss 0.02816 | ppl     1.03\n",
      "| epoch  47 |   212/  269 batches |  lr 0.001935 | 10434.29 ms | loss 0.01059 | ppl     1.01\n",
      "| epoch  47 |   265/  269 batches |  lr 0.001935 | 5240.08 ms | loss 0.01456 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 8.9e+02s | valid loss 0.12214 | valid ppl     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    53/  269 batches |  lr 0.001896 | 859.22 ms | loss 0.02549 | ppl     1.03\n",
      "| epoch  48 |   106/  269 batches |  lr 0.001896 | 873.56 ms | loss 0.03418 | ppl     1.03\n",
      "| epoch  48 |   159/  269 batches |  lr 0.001896 | 1092.77 ms | loss 0.02731 | ppl     1.03\n",
      "| epoch  48 |   212/  269 batches |  lr 0.001896 | 3425.39 ms | loss 0.01200 | ppl     1.01\n",
      "| epoch  48 |   265/  269 batches |  lr 0.001896 | 11169.35 ms | loss 0.01246 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 9.6e+02s | valid loss 0.10555 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    53/  269 batches |  lr 0.001858 | 2256.27 ms | loss 0.02161 | ppl     1.02\n",
      "| epoch  49 |   106/  269 batches |  lr 0.001858 | 1122.41 ms | loss 0.02823 | ppl     1.03\n",
      "| epoch  49 |   159/  269 batches |  lr 0.001858 | 3782.11 ms | loss 0.02695 | ppl     1.03\n",
      "| epoch  49 |   212/  269 batches |  lr 0.001858 | 10895.14 ms | loss 0.01475 | ppl     1.01\n",
      "| epoch  49 |   265/  269 batches |  lr 0.001858 | 8124.36 ms | loss 0.01507 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 1.4e+03s | valid loss 0.09965 | valid ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    53/  269 batches |  lr 0.001821 | 24991.51 ms | loss 0.02072 | ppl     1.02\n",
      "| epoch  50 |   106/  269 batches |  lr 0.001821 | 12378.80 ms | loss 0.02251 | ppl     1.02\n",
      "| epoch  50 |   159/  269 batches |  lr 0.001821 | 2336.72 ms | loss 0.02501 | ppl     1.03\n",
      "| epoch  50 |   212/  269 batches |  lr 0.001821 | 6196.80 ms | loss 0.01703 | ppl     1.02\n",
      "| epoch  50 |   265/  269 batches |  lr 0.001821 | 5617.09 ms | loss 0.01340 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 3.3e+03s | valid loss 0.08231 | valid ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    53/  269 batches |  lr 0.001784 | 1188.10 ms | loss 0.02119 | ppl     1.02\n",
      "| epoch  51 |   106/  269 batches |  lr 0.001784 | 1352.65 ms | loss 0.02858 | ppl     1.03\n",
      "| epoch  51 |   159/  269 batches |  lr 0.001784 | 5323.40 ms | loss 0.02103 | ppl     1.02\n",
      "| epoch  51 |   212/  269 batches |  lr 0.001784 | 1093.79 ms | loss 0.01609 | ppl     1.02\n",
      "| epoch  51 |   265/  269 batches |  lr 0.001784 | 1364.09 ms | loss 0.01244 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 5.6e+02s | valid loss 0.06975 | valid ppl     1.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    53/  269 batches |  lr 0.001749 | 24028.15 ms | loss 0.01789 | ppl     1.02\n",
      "| epoch  52 |   106/  269 batches |  lr 0.001749 | 2956.41 ms | loss 0.02596 | ppl     1.03\n",
      "| epoch  52 |   159/  269 batches |  lr 0.001749 | 17858.21 ms | loss 0.01704 | ppl     1.02\n",
      "| epoch  52 |   212/  269 batches |  lr 0.001749 | 4677.36 ms | loss 0.01411 | ppl     1.01\n",
      "| epoch  52 |   265/  269 batches |  lr 0.001749 | 2727.66 ms | loss 0.01117 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 2.8e+03s | valid loss 0.05759 | valid ppl     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    53/  269 batches |  lr 0.001714 | 3324.57 ms | loss 0.01576 | ppl     1.02\n",
      "| epoch  53 |   106/  269 batches |  lr 0.001714 | 1053.62 ms | loss 0.01852 | ppl     1.02\n",
      "| epoch  53 |   159/  269 batches |  lr 0.001714 | 4158.52 ms | loss 0.01418 | ppl     1.01\n",
      "| epoch  53 |   212/  269 batches |  lr 0.001714 | 5972.70 ms | loss 0.01267 | ppl     1.01\n",
      "| epoch  53 |   265/  269 batches |  lr 0.001714 | 2893.01 ms | loss 0.01062 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 9.3e+02s | valid loss 0.04726 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    53/  269 batches |  lr 0.001679 | 9499.00 ms | loss 0.01470 | ppl     1.01\n",
      "| epoch  54 |   106/  269 batches |  lr 0.001679 | 3699.74 ms | loss 0.01556 | ppl     1.02\n",
      "| epoch  54 |   159/  269 batches |  lr 0.001679 | 1093.75 ms | loss 0.01409 | ppl     1.01\n",
      "| epoch  54 |   212/  269 batches |  lr 0.001679 | 2067.67 ms | loss 0.01181 | ppl     1.01\n",
      "| epoch  54 |   265/  269 batches |  lr 0.001679 | 2737.58 ms | loss 0.01031 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 1e+03s | valid loss 0.04479 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    53/  269 batches |  lr 0.001646 | 822.94 ms | loss 0.01369 | ppl     1.01\n",
      "| epoch  55 |   106/  269 batches |  lr 0.001646 | 5542.52 ms | loss 0.01433 | ppl     1.01\n",
      "| epoch  55 |   159/  269 batches |  lr 0.001646 | 3933.46 ms | loss 0.01271 | ppl     1.01\n",
      "| epoch  55 |   212/  269 batches |  lr 0.001646 | 913.71 ms | loss 0.01265 | ppl     1.01\n",
      "| epoch  55 |   265/  269 batches |  lr 0.001646 | 1649.43 ms | loss 0.01043 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 7.9e+02s | valid loss 0.04538 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    53/  269 batches |  lr 0.001613 | 10960.64 ms | loss 0.01423 | ppl     1.01\n",
      "| epoch  56 |   106/  269 batches |  lr 0.001613 | 3665.08 ms | loss 0.01446 | ppl     1.01\n",
      "| epoch  56 |   159/  269 batches |  lr 0.001613 | 4316.99 ms | loss 0.01255 | ppl     1.01\n",
      "| epoch  56 |   212/  269 batches |  lr 0.001613 | 3185.97 ms | loss 0.01151 | ppl     1.01\n",
      "| epoch  56 |   265/  269 batches |  lr 0.001613 | 825.44 ms | loss 0.01014 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 1.4e+03s | valid loss 0.04283 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    53/  269 batches |  lr 0.001581 | 1633.39 ms | loss 0.01342 | ppl     1.01\n",
      "| epoch  57 |   106/  269 batches |  lr 0.001581 | 1896.42 ms | loss 0.01543 | ppl     1.02\n",
      "| epoch  57 |   159/  269 batches |  lr 0.001581 | 1455.12 ms | loss 0.01261 | ppl     1.01\n",
      "| epoch  57 |   212/  269 batches |  lr 0.001581 | 6552.71 ms | loss 0.01215 | ppl     1.01\n",
      "| epoch  57 |   265/  269 batches |  lr 0.001581 | 1123.49 ms | loss 0.01008 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 6.8e+02s | valid loss 0.04211 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    53/  269 batches |  lr 0.001549 | 8790.86 ms | loss 0.01359 | ppl     1.01\n",
      "| epoch  58 |   106/  269 batches |  lr 0.001549 | 6555.79 ms | loss 0.01295 | ppl     1.01\n",
      "| epoch  58 |   159/  269 batches |  lr 0.001549 | 25677.12 ms | loss 0.01216 | ppl     1.01\n",
      "| epoch  58 |   212/  269 batches |  lr 0.001549 | 5853.83 ms | loss 0.01160 | ppl     1.01\n",
      "| epoch  58 |   265/  269 batches |  lr 0.001549 | 11871.74 ms | loss 0.01020 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 3.2e+03s | valid loss 0.03507 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    53/  269 batches |  lr 0.001518 | 4902.15 ms | loss 0.01402 | ppl     1.01\n",
      "| epoch  59 |   106/  269 batches |  lr 0.001518 | 17839.82 ms | loss 0.01295 | ppl     1.01\n",
      "| epoch  59 |   159/  269 batches |  lr 0.001518 | 23175.58 ms | loss 0.01393 | ppl     1.01\n",
      "| epoch  59 |   212/  269 batches |  lr 0.001518 | 34831.61 ms | loss 0.01125 | ppl     1.01\n",
      "| epoch  59 |   265/  269 batches |  lr 0.001518 | 16579.27 ms | loss 0.00981 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 5.2e+03s | valid loss 0.03120 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    53/  269 batches |  lr 0.001488 | 18789.00 ms | loss 0.01441 | ppl     1.01\n",
      "| epoch  60 |   106/  269 batches |  lr 0.001488 | 19286.63 ms | loss 0.01391 | ppl     1.01\n",
      "| epoch  60 |   159/  269 batches |  lr 0.001488 | 20108.97 ms | loss 0.01368 | ppl     1.01\n",
      "| epoch  60 |   212/  269 batches |  lr 0.001488 | 4782.48 ms | loss 0.01184 | ppl     1.01\n",
      "| epoch  60 |   265/  269 batches |  lr 0.001488 | 4580.69 ms | loss 0.00911 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 5.5e+03s | valid loss 0.03183 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |    53/  269 batches |  lr 0.001458 | 15072.84 ms | loss 0.01427 | ppl     1.01\n",
      "| epoch  61 |   106/  269 batches |  lr 0.001458 | 17734.68 ms | loss 0.01246 | ppl     1.01\n",
      "| epoch  61 |   159/  269 batches |  lr 0.001458 | 17721.99 ms | loss 0.01314 | ppl     1.01\n",
      "| epoch  61 |   212/  269 batches |  lr 0.001458 | 17597.00 ms | loss 0.01098 | ppl     1.01\n",
      "| epoch  61 |   265/  269 batches |  lr 0.001458 | 17395.24 ms | loss 0.00948 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 4.5e+03s | valid loss 0.03014 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    53/  269 batches |  lr 0.001429 | 15896.48 ms | loss 0.01387 | ppl     1.01\n",
      "| epoch  62 |   106/  269 batches |  lr 0.001429 | 34711.81 ms | loss 0.01154 | ppl     1.01\n",
      "| epoch  62 |   159/  269 batches |  lr 0.001429 | 11238.11 ms | loss 0.01276 | ppl     1.01\n",
      "| epoch  62 |   212/  269 batches |  lr 0.001429 | 17530.68 ms | loss 0.01092 | ppl     1.01\n",
      "| epoch  62 |   265/  269 batches |  lr 0.001429 | 844.86 ms | loss 0.00908 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 4.5e+03s | valid loss 0.02957 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    53/  269 batches |  lr 0.001400 | 17577.49 ms | loss 0.01345 | ppl     1.01\n",
      "| epoch  63 |   106/  269 batches |  lr 0.001400 | 9415.22 ms | loss 0.01174 | ppl     1.01\n",
      "| epoch  63 |   159/  269 batches |  lr 0.001400 | 25955.13 ms | loss 0.01331 | ppl     1.01\n",
      "| epoch  63 |   212/  269 batches |  lr 0.001400 | 368.90 ms | loss 0.01067 | ppl     1.01\n",
      "| epoch  63 |   265/  269 batches |  lr 0.001400 | 16059.37 ms | loss 0.00918 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 3.7e+03s | valid loss 0.02802 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    53/  269 batches |  lr 0.001372 | 17544.41 ms | loss 0.01258 | ppl     1.01\n",
      "| epoch  64 |   106/  269 batches |  lr 0.001372 | 1404.33 ms | loss 0.01161 | ppl     1.01\n",
      "| epoch  64 |   159/  269 batches |  lr 0.001372 | 34616.14 ms | loss 0.01157 | ppl     1.01\n",
      "| epoch  64 |   212/  269 batches |  lr 0.001372 | 15117.17 ms | loss 0.01016 | ppl     1.01\n",
      "| epoch  64 |   265/  269 batches |  lr 0.001372 | 327.28 ms | loss 0.00903 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 3.7e+03s | valid loss 0.03336 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    53/  269 batches |  lr 0.001345 | 18797.11 ms | loss 0.01273 | ppl     1.01\n",
      "| epoch  65 |   106/  269 batches |  lr 0.001345 | 34537.10 ms | loss 0.01152 | ppl     1.01\n",
      "| epoch  65 |   159/  269 batches |  lr 0.001345 | 15295.89 ms | loss 0.01105 | ppl     1.01\n",
      "| epoch  65 |   212/  269 batches |  lr 0.001345 | 402.40 ms | loss 0.00996 | ppl     1.01\n",
      "| epoch  65 |   265/  269 batches |  lr 0.001345 | 17360.28 ms | loss 0.00836 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 4.6e+03s | valid loss 0.03182 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    53/  269 batches |  lr 0.001318 | 17482.75 ms | loss 0.01371 | ppl     1.01\n",
      "| epoch  66 |   106/  269 batches |  lr 0.001318 | 26621.76 ms | loss 0.01052 | ppl     1.01\n",
      "| epoch  66 |   159/  269 batches |  lr 0.001318 | 4086.68 ms | loss 0.01122 | ppl     1.01\n",
      "| epoch  66 |   212/  269 batches |  lr 0.001318 | 4376.37 ms | loss 0.01000 | ppl     1.01\n",
      "| epoch  66 |   265/  269 batches |  lr 0.001318 | 16470.23 ms | loss 0.00851 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 3.7e+03s | valid loss 0.03589 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    53/  269 batches |  lr 0.001292 | 4037.24 ms | loss 0.01293 | ppl     1.01\n",
      "| epoch  67 |   106/  269 batches |  lr 0.001292 | 533.07 ms | loss 0.01051 | ppl     1.01\n",
      "| epoch  67 |   159/  269 batches |  lr 0.001292 | 1041.90 ms | loss 0.01214 | ppl     1.01\n",
      "| epoch  67 |   212/  269 batches |  lr 0.001292 | 768.40 ms | loss 0.00973 | ppl     1.01\n",
      "| epoch  67 |   265/  269 batches |  lr 0.001292 | 471.02 ms | loss 0.00806 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 3.7e+02s | valid loss 0.03940 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |    53/  269 batches |  lr 0.001266 | 2081.64 ms | loss 0.01335 | ppl     1.01\n",
      "| epoch  68 |   106/  269 batches |  lr 0.001266 | 884.15 ms | loss 0.01090 | ppl     1.01\n",
      "| epoch  68 |   159/  269 batches |  lr 0.001266 | 383.75 ms | loss 0.01190 | ppl     1.01\n",
      "| epoch  68 |   212/  269 batches |  lr 0.001266 | 716.55 ms | loss 0.01014 | ppl     1.01\n",
      "| epoch  68 |   265/  269 batches |  lr 0.001266 | 405.15 ms | loss 0.00824 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 2.5e+02s | valid loss 0.03582 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |    53/  269 batches |  lr 0.001240 | 3865.57 ms | loss 0.01248 | ppl     1.01\n",
      "| epoch  69 |   106/  269 batches |  lr 0.001240 | 477.03 ms | loss 0.01083 | ppl     1.01\n",
      "| epoch  69 |   159/  269 batches |  lr 0.001240 | 2699.55 ms | loss 0.01099 | ppl     1.01\n",
      "| epoch  69 |   212/  269 batches |  lr 0.001240 | 4127.48 ms | loss 0.01090 | ppl     1.01\n",
      "| epoch  69 |   265/  269 batches |  lr 0.001240 | 654.86 ms | loss 0.00868 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 6.9e+02s | valid loss 0.03849 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |    53/  269 batches |  lr 0.001216 | 594.41 ms | loss 0.01160 | ppl     1.01\n",
      "| epoch  70 |   106/  269 batches |  lr 0.001216 | 737.53 ms | loss 0.01103 | ppl     1.01\n",
      "| epoch  70 |   159/  269 batches |  lr 0.001216 | 2240.46 ms | loss 0.01074 | ppl     1.01\n",
      "| epoch  70 |   212/  269 batches |  lr 0.001216 | 617.49 ms | loss 0.01104 | ppl     1.01\n",
      "| epoch  70 |   265/  269 batches |  lr 0.001216 | 533.82 ms | loss 0.00926 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 3e+02s | valid loss 0.03176 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |    53/  269 batches |  lr 0.001191 | 1003.82 ms | loss 0.01193 | ppl     1.01\n",
      "| epoch  71 |   106/  269 batches |  lr 0.001191 | 1532.94 ms | loss 0.01047 | ppl     1.01\n",
      "| epoch  71 |   159/  269 batches |  lr 0.001191 | 813.49 ms | loss 0.01120 | ppl     1.01\n",
      "| epoch  71 |   212/  269 batches |  lr 0.001191 | 1194.57 ms | loss 0.01085 | ppl     1.01\n",
      "| epoch  71 |   265/  269 batches |  lr 0.001191 | 16562.38 ms | loss 0.00880 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 1.1e+03s | valid loss 0.03104 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |    53/  269 batches |  lr 0.001167 | 34767.45 ms | loss 0.01260 | ppl     1.01\n",
      "| epoch  72 |   106/  269 batches |  lr 0.001167 | 17470.25 ms | loss 0.01064 | ppl     1.01\n",
      "| epoch  72 |   159/  269 batches |  lr 0.001167 | 418.79 ms | loss 0.01202 | ppl     1.01\n",
      "| epoch  72 |   212/  269 batches |  lr 0.001167 | 16408.40 ms | loss 0.01076 | ppl     1.01\n",
      "| epoch  72 |   265/  269 batches |  lr 0.001167 | 17424.99 ms | loss 0.00940 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 5.5e+03s | valid loss 0.02833 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |    53/  269 batches |  lr 0.001144 | 17622.70 ms | loss 0.01248 | ppl     1.01\n",
      "| epoch  73 |   106/  269 batches |  lr 0.001144 | 16697.29 ms | loss 0.01081 | ppl     1.01\n",
      "| epoch  73 |   159/  269 batches |  lr 0.001144 | 17805.50 ms | loss 0.01077 | ppl     1.01\n",
      "| epoch  73 |   212/  269 batches |  lr 0.001144 | 34874.47 ms | loss 0.01155 | ppl     1.01\n",
      "| epoch  73 |   265/  269 batches |  lr 0.001144 | 33692.21 ms | loss 0.00947 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 6.4e+03s | valid loss 0.02567 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |    53/  269 batches |  lr 0.001121 | 34717.64 ms | loss 0.01222 | ppl     1.01\n",
      "| epoch  74 |   106/  269 batches |  lr 0.001121 | 16759.28 ms | loss 0.01056 | ppl     1.01\n",
      "| epoch  74 |   159/  269 batches |  lr 0.001121 | 409.65 ms | loss 0.01107 | ppl     1.01\n",
      "| epoch  74 |   212/  269 batches |  lr 0.001121 | 2323.95 ms | loss 0.01074 | ppl     1.01\n",
      "| epoch  74 |   265/  269 batches |  lr 0.001121 | 2605.07 ms | loss 0.01002 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 3e+03s | valid loss 0.02101 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |    53/  269 batches |  lr 0.001099 | 2799.57 ms | loss 0.01206 | ppl     1.01\n",
      "| epoch  75 |   106/  269 batches |  lr 0.001099 | 1532.65 ms | loss 0.01005 | ppl     1.01\n",
      "| epoch  75 |   159/  269 batches |  lr 0.001099 | 2229.84 ms | loss 0.01120 | ppl     1.01\n",
      "| epoch  75 |   212/  269 batches |  lr 0.001099 | 2008.01 ms | loss 0.01066 | ppl     1.01\n",
      "| epoch  75 |   265/  269 batches |  lr 0.001099 | 430.98 ms | loss 0.01017 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 4.8e+02s | valid loss 0.01861 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |    53/  269 batches |  lr 0.001077 | 803.78 ms | loss 0.01091 | ppl     1.01\n",
      "| epoch  76 |   106/  269 batches |  lr 0.001077 | 1361.98 ms | loss 0.00953 | ppl     1.01\n",
      "| epoch  76 |   159/  269 batches |  lr 0.001077 | 891.51 ms | loss 0.01093 | ppl     1.01\n",
      "| epoch  76 |   212/  269 batches |  lr 0.001077 | 2844.28 ms | loss 0.01012 | ppl     1.01\n",
      "| epoch  76 |   265/  269 batches |  lr 0.001077 | 454.75 ms | loss 0.00954 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 3.4e+02s | valid loss 0.01914 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |    53/  269 batches |  lr 0.001055 | 1099.53 ms | loss 0.01143 | ppl     1.01\n",
      "| epoch  77 |   106/  269 batches |  lr 0.001055 | 1533.33 ms | loss 0.00941 | ppl     1.01\n",
      "| epoch  77 |   159/  269 batches |  lr 0.001055 | 650.25 ms | loss 0.01017 | ppl     1.01\n",
      "| epoch  77 |   212/  269 batches |  lr 0.001055 | 928.99 ms | loss 0.01060 | ppl     1.01\n",
      "| epoch  77 |   265/  269 batches |  lr 0.001055 | 602.86 ms | loss 0.01070 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 2.8e+02s | valid loss 0.01712 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |    53/  269 batches |  lr 0.001034 | 1220.22 ms | loss 0.01135 | ppl     1.01\n",
      "| epoch  78 |   106/  269 batches |  lr 0.001034 | 911.09 ms | loss 0.00901 | ppl     1.01\n",
      "| epoch  78 |   159/  269 batches |  lr 0.001034 | 777.33 ms | loss 0.01014 | ppl     1.01\n",
      "| epoch  78 |   212/  269 batches |  lr 0.001034 | 1015.54 ms | loss 0.01044 | ppl     1.01\n",
      "| epoch  78 |   265/  269 batches |  lr 0.001034 | 1271.37 ms | loss 0.01012 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 2.8e+02s | valid loss 0.01653 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |    53/  269 batches |  lr 0.001014 | 2007.23 ms | loss 0.01033 | ppl     1.01\n",
      "| epoch  79 |   106/  269 batches |  lr 0.001014 | 660.63 ms | loss 0.00880 | ppl     1.01\n",
      "| epoch  79 |   159/  269 batches |  lr 0.001014 | 435.16 ms | loss 0.01004 | ppl     1.01\n",
      "| epoch  79 |   212/  269 batches |  lr 0.001014 | 757.58 ms | loss 0.01059 | ppl     1.01\n",
      "| epoch  79 |   265/  269 batches |  lr 0.001014 | 777.46 ms | loss 0.00951 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 2.5e+02s | valid loss 0.01581 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |    53/  269 batches |  lr 0.000993 | 867.18 ms | loss 0.01013 | ppl     1.01\n",
      "| epoch  80 |   106/  269 batches |  lr 0.000993 | 1028.80 ms | loss 0.00857 | ppl     1.01\n",
      "| epoch  80 |   159/  269 batches |  lr 0.000993 | 1491.66 ms | loss 0.01029 | ppl     1.01\n",
      "| epoch  80 |   212/  269 batches |  lr 0.000993 | 2973.95 ms | loss 0.01074 | ppl     1.01\n",
      "| epoch  80 |   265/  269 batches |  lr 0.000993 | 915.36 ms | loss 0.00960 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 7.4e+02s | valid loss 0.01502 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |    53/  269 batches |  lr 0.000973 | 1980.33 ms | loss 0.01030 | ppl     1.01\n",
      "| epoch  81 |   106/  269 batches |  lr 0.000973 | 2019.76 ms | loss 0.00878 | ppl     1.01\n",
      "| epoch  81 |   159/  269 batches |  lr 0.000973 | 1497.34 ms | loss 0.01016 | ppl     1.01\n",
      "| epoch  81 |   212/  269 batches |  lr 0.000973 | 421.35 ms | loss 0.01034 | ppl     1.01\n",
      "| epoch  81 |   265/  269 batches |  lr 0.000973 | 704.47 ms | loss 0.01006 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 3.6e+02s | valid loss 0.01554 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |    53/  269 batches |  lr 0.000954 | 714.47 ms | loss 0.01032 | ppl     1.01\n",
      "| epoch  82 |   106/  269 batches |  lr 0.000954 | 515.49 ms | loss 0.00898 | ppl     1.01\n",
      "| epoch  82 |   159/  269 batches |  lr 0.000954 | 1330.20 ms | loss 0.00942 | ppl     1.01\n",
      "| epoch  82 |   212/  269 batches |  lr 0.000954 | 2892.02 ms | loss 0.01064 | ppl     1.01\n",
      "| epoch  82 |   265/  269 batches |  lr 0.000954 | 826.12 ms | loss 0.01088 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 3.4e+02s | valid loss 0.01727 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |    53/  269 batches |  lr 0.000935 | 560.16 ms | loss 0.01015 | ppl     1.01\n",
      "| epoch  83 |   106/  269 batches |  lr 0.000935 | 2976.74 ms | loss 0.00874 | ppl     1.01\n",
      "| epoch  83 |   159/  269 batches |  lr 0.000935 | 528.93 ms | loss 0.01011 | ppl     1.01\n",
      "| epoch  83 |   212/  269 batches |  lr 0.000935 | 2149.96 ms | loss 0.00981 | ppl     1.01\n",
      "| epoch  83 |   265/  269 batches |  lr 0.000935 | 17910.30 ms | loss 0.00964 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 1.3e+03s | valid loss 0.01387 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |    53/  269 batches |  lr 0.000916 | 17441.08 ms | loss 0.01012 | ppl     1.01\n",
      "| epoch  84 |   106/  269 batches |  lr 0.000916 | 17605.07 ms | loss 0.00902 | ppl     1.01\n",
      "| epoch  84 |   159/  269 batches |  lr 0.000916 | 912.91 ms | loss 0.00957 | ppl     1.01\n",
      "| epoch  84 |   212/  269 batches |  lr 0.000916 | 14650.17 ms | loss 0.01100 | ppl     1.01\n",
      "| epoch  84 |   265/  269 batches |  lr 0.000916 | 16059.39 ms | loss 0.01070 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 4.1e+03s | valid loss 0.01232 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |    53/  269 batches |  lr 0.000898 | 10868.35 ms | loss 0.01139 | ppl     1.01\n",
      "| epoch  85 |   106/  269 batches |  lr 0.000898 | 30707.28 ms | loss 0.00984 | ppl     1.01\n",
      "| epoch  85 |   159/  269 batches |  lr 0.000898 | 18721.21 ms | loss 0.01067 | ppl     1.01\n",
      "| epoch  85 |   212/  269 batches |  lr 0.000898 | 408.96 ms | loss 0.01052 | ppl     1.01\n",
      "| epoch  85 |   265/  269 batches |  lr 0.000898 | 24685.28 ms | loss 0.01044 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 4.5e+03s | valid loss 0.01295 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |    53/  269 batches |  lr 0.000880 | 25471.80 ms | loss 0.01070 | ppl     1.01\n",
      "| epoch  86 |   106/  269 batches |  lr 0.000880 | 17377.81 ms | loss 0.00929 | ppl     1.01\n",
      "| epoch  86 |   159/  269 batches |  lr 0.000880 | 23640.64 ms | loss 0.01021 | ppl     1.01\n",
      "| epoch  86 |   212/  269 batches |  lr 0.000880 | 338.39 ms | loss 0.00957 | ppl     1.01\n",
      "| epoch  86 |   265/  269 batches |  lr 0.000880 | 7285.41 ms | loss 0.00981 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 4.8e+03s | valid loss 0.01128 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |    53/  269 batches |  lr 0.000862 | 419.36 ms | loss 0.01001 | ppl     1.01\n",
      "| epoch  87 |   106/  269 batches |  lr 0.000862 | 3164.09 ms | loss 0.00899 | ppl     1.01\n",
      "| epoch  87 |   159/  269 batches |  lr 0.000862 | 17516.70 ms | loss 0.00952 | ppl     1.01\n",
      "| epoch  87 |   212/  269 batches |  lr 0.000862 | 17446.02 ms | loss 0.00946 | ppl     1.01\n",
      "| epoch  87 |   265/  269 batches |  lr 0.000862 | 20264.65 ms | loss 0.00964 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 3.1e+03s | valid loss 0.01099 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |    53/  269 batches |  lr 0.000845 | 13157.94 ms | loss 0.00920 | ppl     1.01\n",
      "| epoch  88 |   106/  269 batches |  lr 0.000845 | 11966.18 ms | loss 0.00906 | ppl     1.01\n",
      "| epoch  88 |   159/  269 batches |  lr 0.000845 | 4655.72 ms | loss 0.00975 | ppl     1.01\n",
      "| epoch  88 |   212/  269 batches |  lr 0.000845 | 2964.48 ms | loss 0.00920 | ppl     1.01\n",
      "| epoch  88 |   265/  269 batches |  lr 0.000845 | 10164.04 ms | loss 0.00953 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 2.4e+03s | valid loss 0.01048 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |    53/  269 batches |  lr 0.000828 | 24904.87 ms | loss 0.00940 | ppl     1.01\n",
      "| epoch  89 |   106/  269 batches |  lr 0.000828 | 11939.02 ms | loss 0.00872 | ppl     1.01\n",
      "| epoch  89 |   159/  269 batches |  lr 0.000828 | 17520.49 ms | loss 0.00988 | ppl     1.01\n",
      "| epoch  89 |   212/  269 batches |  lr 0.000828 | 443.31 ms | loss 0.00951 | ppl     1.01\n",
      "| epoch  89 |   265/  269 batches |  lr 0.000828 | 17453.14 ms | loss 0.00931 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 4.4e+03s | valid loss 0.00998 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |    53/  269 batches |  lr 0.000812 | 715.66 ms | loss 0.00953 | ppl     1.01\n",
      "| epoch  90 |   106/  269 batches |  lr 0.000812 | 6286.45 ms | loss 0.00822 | ppl     1.01\n",
      "| epoch  90 |   159/  269 batches |  lr 0.000812 | 16822.42 ms | loss 0.00981 | ppl     1.01\n",
      "| epoch  90 |   212/  269 batches |  lr 0.000812 | 17432.34 ms | loss 0.00865 | ppl     1.01\n",
      "| epoch  90 |   265/  269 batches |  lr 0.000812 | 17356.05 ms | loss 0.00901 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 3.3e+03s | valid loss 0.00994 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |    53/  269 batches |  lr 0.000795 | 792.85 ms | loss 0.00943 | ppl     1.01\n",
      "| epoch  91 |   106/  269 batches |  lr 0.000795 | 482.46 ms | loss 0.00820 | ppl     1.01\n",
      "| epoch  91 |   159/  269 batches |  lr 0.000795 | 1579.67 ms | loss 0.00915 | ppl     1.01\n",
      "| epoch  91 |   212/  269 batches |  lr 0.000795 | 901.46 ms | loss 0.00894 | ppl     1.01\n",
      "| epoch  91 |   265/  269 batches |  lr 0.000795 | 1137.11 ms | loss 0.00874 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 2.7e+02s | valid loss 0.00968 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |    53/  269 batches |  lr 0.000779 | 1305.74 ms | loss 0.00885 | ppl     1.01\n",
      "| epoch  92 |   106/  269 batches |  lr 0.000779 | 424.21 ms | loss 0.00817 | ppl     1.01\n",
      "| epoch  92 |   159/  269 batches |  lr 0.000779 | 7041.30 ms | loss 0.00932 | ppl     1.01\n",
      "| epoch  92 |   212/  269 batches |  lr 0.000779 | 2086.52 ms | loss 0.00862 | ppl     1.01\n",
      "| epoch  92 |   265/  269 batches |  lr 0.000779 | 1798.77 ms | loss 0.00892 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 6.8e+02s | valid loss 0.00968 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |    53/  269 batches |  lr 0.000764 | 1543.14 ms | loss 0.00864 | ppl     1.01\n",
      "| epoch  93 |   106/  269 batches |  lr 0.000764 | 2636.23 ms | loss 0.00802 | ppl     1.01\n",
      "| epoch  93 |   159/  269 batches |  lr 0.000764 | 1162.66 ms | loss 0.00864 | ppl     1.01\n",
      "| epoch  93 |   212/  269 batches |  lr 0.000764 | 1677.92 ms | loss 0.00855 | ppl     1.01\n",
      "| epoch  93 |   265/  269 batches |  lr 0.000764 | 900.73 ms | loss 0.00850 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 4.8e+02s | valid loss 0.00962 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |    53/  269 batches |  lr 0.000749 | 473.22 ms | loss 0.00880 | ppl     1.01\n",
      "| epoch  94 |   106/  269 batches |  lr 0.000749 | 1333.62 ms | loss 0.00782 | ppl     1.01\n",
      "| epoch  94 |   159/  269 batches |  lr 0.000749 | 1979.97 ms | loss 0.00925 | ppl     1.01\n",
      "| epoch  94 |   212/  269 batches |  lr 0.000749 | 9376.93 ms | loss 0.00820 | ppl     1.01\n",
      "| epoch  94 |   265/  269 batches |  lr 0.000749 | 7269.97 ms | loss 0.00849 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 1.1e+03s | valid loss 0.00969 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |    53/  269 batches |  lr 0.000734 | 15830.42 ms | loss 0.00871 | ppl     1.01\n",
      "| epoch  95 |   106/  269 batches |  lr 0.000734 | 975.26 ms | loss 0.00770 | ppl     1.01\n",
      "| epoch  95 |   159/  269 batches |  lr 0.000734 | 608.91 ms | loss 0.00882 | ppl     1.01\n",
      "| epoch  95 |   212/  269 batches |  lr 0.000734 | 7362.52 ms | loss 0.00870 | ppl     1.01\n",
      "| epoch  95 |   265/  269 batches |  lr 0.000734 | 20695.77 ms | loss 0.00914 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 2.4e+03s | valid loss 0.00951 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |    53/  269 batches |  lr 0.000719 | 23775.02 ms | loss 0.00870 | ppl     1.01\n",
      "| epoch  96 |   106/  269 batches |  lr 0.000719 | 6135.17 ms | loss 0.00798 | ppl     1.01\n",
      "| epoch  96 |   159/  269 batches |  lr 0.000719 | 11417.49 ms | loss 0.00901 | ppl     1.01\n",
      "| epoch  96 |   212/  269 batches |  lr 0.000719 | 18186.47 ms | loss 0.00857 | ppl     1.01\n",
      "| epoch  96 |   265/  269 batches |  lr 0.000719 | 917.53 ms | loss 0.00884 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 3.2e+03s | valid loss 0.00932 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |    53/  269 batches |  lr 0.000705 | 2084.62 ms | loss 0.00891 | ppl     1.01\n",
      "| epoch  97 |   106/  269 batches |  lr 0.000705 | 1664.23 ms | loss 0.00781 | ppl     1.01\n",
      "| epoch  97 |   159/  269 batches |  lr 0.000705 | 10570.09 ms | loss 0.00868 | ppl     1.01\n",
      "| epoch  97 |   212/  269 batches |  lr 0.000705 | 9441.43 ms | loss 0.00850 | ppl     1.01\n",
      "| epoch  97 |   265/  269 batches |  lr 0.000705 | 17460.27 ms | loss 0.00891 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 3.1e+03s | valid loss 0.00925 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |    53/  269 batches |  lr 0.000690 | 4356.57 ms | loss 0.00838 | ppl     1.01\n",
      "| epoch  98 |   106/  269 batches |  lr 0.000690 | 17828.81 ms | loss 0.00754 | ppl     1.01\n",
      "| epoch  98 |   159/  269 batches |  lr 0.000690 | 19982.67 ms | loss 0.00861 | ppl     1.01\n",
      "| epoch  98 |   212/  269 batches |  lr 0.000690 | 17565.23 ms | loss 0.00848 | ppl     1.01\n",
      "| epoch  98 |   265/  269 batches |  lr 0.000690 | 10048.42 ms | loss 0.00824 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 4.6e+03s | valid loss 0.00895 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |    53/  269 batches |  lr 0.000677 | 9318.52 ms | loss 0.00802 | ppl     1.01\n",
      "| epoch  99 |   106/  269 batches |  lr 0.000677 | 34894.07 ms | loss 0.00767 | ppl     1.01\n",
      "| epoch  99 |   159/  269 batches |  lr 0.000677 | 17967.84 ms | loss 0.00894 | ppl     1.01\n",
      "| epoch  99 |   212/  269 batches |  lr 0.000677 | 5125.81 ms | loss 0.00865 | ppl     1.01\n",
      "| epoch  99 |   265/  269 batches |  lr 0.000677 | 26190.36 ms | loss 0.00822 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 5e+03s | valid loss 0.00892 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |    53/  269 batches |  lr 0.000663 | 6537.69 ms | loss 0.00841 | ppl     1.01\n",
      "| epoch 100 |   106/  269 batches |  lr 0.000663 | 1189.51 ms | loss 0.00770 | ppl     1.01\n",
      "| epoch 100 |   159/  269 batches |  lr 0.000663 | 11904.14 ms | loss 0.00841 | ppl     1.01\n",
      "| epoch 100 |   212/  269 batches |  lr 0.000663 | 3841.82 ms | loss 0.00805 | ppl     1.01\n",
      "| epoch 100 |   265/  269 batches |  lr 0.000663 | 2735.84 ms | loss 0.00798 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 6.9e+04s | valid loss 0.00880 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data() \n",
    "#  data preprocessing function \n",
    "\n",
    "model = TransAm().to(device)\n",
    "# instantiate transformer model \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# define loss function \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# set up optimizer (Adam with decoupled weight decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.98)\n",
    "# create learning rate scheduler \n",
    "# at every step size = 1, epoch, it multiples the LR by gamma=98 to encourage more stable convergence\n",
    "\n",
    "best_val_loss = float(\"inf\") # track the lowest validation loss\n",
    "best_model = None # best performing model \n",
    "\n",
    "for epoch in range(1, epochs + 1): # training loop from epoch = 1\n",
    "    epoch_start_time = time.time()  # start time for logging \n",
    "    train(train_data) # train model \n",
    "\n",
    "    if (epoch % 10 == 0): # every 10 epochs \n",
    "        val_loss = plot_and_loss(model, val_data, epoch) # compute val loss and plot model performance\n",
    "        predict_future(model, val_data, 200) # forecast 200 steps in future\n",
    "    else: \n",
    "        val_loss = evaluate(model, val_data)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
