{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b7955b",
   "metadata": {},
   "source": [
    "# Toy Problem - Transformer Application to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50377cbf",
   "metadata": {},
   "source": [
    "https://github.com/oliverguhr/transformer-time-series-prediction/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec12fa0",
   "metadata": {},
   "source": [
    "Description: This example is from the repo above. It contains 2 PyTorch models for a transformer-based time series prediction. The dataset is stored in ./daily-min-temperatures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import time \n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb50495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = source sequence length\n",
    "# T = target sequence length \n",
    "# N = batch size \n",
    "# E = feature number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43280924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 \n",
    "output_window = 5\n",
    "batch_size = 10 \n",
    "lr = 0.005\n",
    "epochs = 100\n",
    "device = torch.device(\"curda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8380cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # define a pytorch module that inherits nn.Module\n",
    "    \"\"\"\n",
    "    Positional encoding layer for transformer model. \n",
    "\n",
    "    Layer injects info about the relative or absolute position of the sequence, without adding learnable parameters. \n",
    "\n",
    "    Uses sin and cos fcns of different frequencies to encode position info. \n",
    "\n",
    "    Args: \n",
    "        d_model (int): dimension of the embedding space \n",
    "        max_len (int, optional): max sequence length supported. Default is 5000 \n",
    "\n",
    "    Attriutes: \n",
    "        pe (Tensor): Fixed positional encoding matrix of space (max_len, 1, d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # the \"Attributes\" part documents the instance variables inside the __init__\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000): # creates init method \n",
    "        super(PositionalEncoding, self).__init__() # super() lets us avoid referring to the base class explicitly\n",
    "        # https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods\n",
    "        pe = torch.zeros(max_len, d_model) # create empty matrix of shape max_len X d_model to hold the positional encodings\n",
    "        # row: position i.e. 0, 1, 2, \n",
    "        # column: dim of the embedding \n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # vector of positions [0, 1, 2, 3, ..., 4999]\n",
    "        # unsqueeze reshapes vector from [max_len,] to [max_len, 1] to enable broadcasting \n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # division term for the sin and cos fcns \n",
    "        # torch.arange(0, d_model, 2).float(): starts at 0, ends at d_model, step size = 2 \n",
    "        # -ln(10000)\n",
    "        # torch.exp = exp \n",
    "        # this comes from \"Attention is all you need\" paper where sin and cos fcns of different frequencies are used where each dimension of the positional encoding corresponds to a sine\n",
    "        # PE at dim i = PE_(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        # PE_(pos, 2i+1) = cos(pos/10000^(2i/d_model)) \n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # at even indices: sin(position * frequency)\n",
    "    \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # at odd indices: cos(position * frequency)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        # pe.unsqueeze(0) == adds a batch dimension so the shape becomes: [1, max_len, d_model]\n",
    "        # .transpose(0,1) == swaps the first and second dimensions such that the new shape is [max_len, 1, d_model]\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        # saving pe tensor. Tensor which is not a parameter, but should be part of the module's state. Used for tensors that need to be on the same device as the module. \n",
    "        # it's a fixed tensor stored with the model and moved to the GPU/CPU automatically \n",
    "        # this is NOT updated during backprop \n",
    "\n",
    "    def forward(self, x): # during the forward pass, x is the input with shape [sequence length, batch_size, d_model]\n",
    "        return x + self.pe[:x.size(0), :] # add the pe for the len of the input, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35741f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USE OF POSITIONAL ENCODING\n",
    "positional_encoder = PositionalEncoding(d_model = 512)\n",
    "sample_x = torch.randn(100, 32, 512) # tensor filled with random numbers from a standard normal distribution of shape [100, 32, 512]\n",
    "sample_encode = positional_encoder(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ff6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -1.1524, -0.2506],\n",
      "         [-0.5461, -0.6302, -0.6347],\n",
      "         [-1.0841, -0.1287, -0.6811]],\n",
      "\n",
      "        [[-0.5518,  1.5398,  1.0036],\n",
      "         [-0.4424,  0.2087,  0.0160],\n",
      "         [ 1.2970, -0.4725,  0.3149]],\n",
      "\n",
      "        [[-0.9780,  0.6038, -1.7178],\n",
      "         [-0.3399, -0.2990,  1.8007],\n",
      "         [ 0.6786,  0.5225, -0.0246]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_x[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63089d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -0.1524, -0.2506],\n",
      "         [-0.5461,  0.3698, -0.6347],\n",
      "         [-1.0841,  0.8713, -0.6811]],\n",
      "\n",
      "        [[ 0.2896,  2.0801,  1.8254],\n",
      "         [ 0.3990,  0.7490,  0.8379],\n",
      "         [ 2.1385,  0.0678,  1.1368]],\n",
      "\n",
      "        [[-0.0687,  0.1877, -0.7814],\n",
      "         [ 0.5694, -0.7151,  2.7371],\n",
      "         [ 1.5879,  0.1063,  0.9118]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_encode[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f201e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module): # defines a model class \n",
    "    def __init__(self, feature_size=250, num_layers=1, dropout=0.1): # initialize model \n",
    "        # feature size: input embedding dimension \n",
    "        # num_layers: transformer layers to stack \n",
    "        # dropout: dropout rate for regularization \n",
    "\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer' # string label \n",
    "        self.src_mask = None # used for sequence masking - important in autoregressive tasks like time-series and language modeling\n",
    "        self.pos_encoder = PositionalEncoding(feature_size) # giving position awareness to the input embeddings before processed by the transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout) # one transformer encoder layer with 10 heads \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers) # stack multiple copies of the encoder layer \n",
    "        self.decoder = nn.Linear(feature_size, 1) # maps the feature size to 1 (in time series forecasting, this maps one number per position)\n",
    "        self.init_weights() # calls init_weights methods to initialize the weights \n",
    "\n",
    "    def init_weights(self): \n",
    "        initrange = 0.1 # define a small numer to set range for random initialization \n",
    "        self.decoder.bias.data.zero_() # bias is the additive constant inside Linear layers. This overwrites all biases to 0 so that only weights matter. Random biases could introduce unwanted drift right at the start so we initialize at 0\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange) # initialize weights between -0.1 and 0.1. Smaller weights help stabilize training early on \n",
    "\n",
    "    def forward(self,src): # the forward pass for the model \n",
    "\n",
    "        # because this is a causal task (like time series or language modeling), we must prevent tokens from seeing the future, so we implement this mask to avoid attention to future tokens\n",
    "        # the mask is a square matrix of size (seq_length, seq_length)\n",
    "\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) !=len(src): # check if a new source mask needs to be created (or a new one)\n",
    "            # if self.src_mask is None then no mask was created yet\n",
    "            #if self.src_mask.size(0) != len(src), the input seq len has changed since the previous iteration therefore we need one of a new size \n",
    "\n",
    "            device = src.device # get the device where the input tensor lives to ensure the mask is on the same device \n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device) \n",
    "            # generate causal mask of len(src) such that the token can attend only to itself and earlier tokens\n",
    "            self.src_mask = mask \n",
    "\n",
    "        src = self.pos_encoder(src) # apply positional encoding to the input embeddings \n",
    "        output = self.transformer_encoder(src, self.src_mask) # pass position-encoded input into the stacked transformer encoder layers \n",
    "        # this is where mlti-head self-attention happens \n",
    "        # each token attends to the previous ones bc of the mask \n",
    "\n",
    "        output = self.decoder(output)\n",
    "        # apply Linear layer to every position \n",
    "        # turn the feature_size vector to a scalar \n",
    "        # this is the model's final prediction at each time step or token \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz): \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1) # create upper triangular matrix of ones, then trnaspose flips it to make lower triangle of 1s \n",
    "        mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0)) # convert 1s/0s into attention scores. mask == 0 ==> future positions, mask == 1 ==> self and past \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda7300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare sequence data for training model\n",
    "\n",
    "def create_inout_sequences(input_data, tw): \n",
    "    # function: slices a time series dataset into overlapping i/o sequences for training \n",
    "    # input_data: 1D array (the time series)\n",
    "    # tw: time window (the sequence length)\n",
    "    # output: list of (input sequence, label sequence) pairs \n",
    "\n",
    "    inout_seq = [] # preallocate memory to hold the (input, label) pairs \n",
    "    L = len(input_data) # stores the length of the sequence \n",
    "    for i in range(L-tw): # loop through the sequence to get all sliding windows of length tw\n",
    "        # stop at L-tw to ensure input_data[i:i+tw] stays within the length of the input_data \n",
    "\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window], output_window * [0])\n",
    "        # input_data[i:i+tw]: gives a window of length tw \n",
    "        # [:-output_window]: removes the last output_window values - therefore we're making the last value(s) we want the model to predict \n",
    "        # output_window * [0]: appends 0s at the end for the same length removed\n",
    "\n",
    "        train_label = input_data[i:i+tw]\n",
    "        # this is the ground truth - the full unmasked windwo including the parts zeroed out \n",
    "\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "        # saves tuple of the masked input and the ground label \n",
    "\n",
    "    return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ff5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates and prepares synthetic time series data for the transformer, including normalizaton and splitting into training and tests sets \n",
    "def get_data(): \n",
    "\n",
    "    # generate synthetic data of a continuous signal \n",
    "    time = np.arange(0, 400, 0.1) # len = 4000 \n",
    "    amplitude = np.sin(time) + np.sin(time * 0.05) + np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
    "    # np.sin(time): main wave \n",
    "    # np.sin(time*0.05): low-frequency drift\n",
    "    # np.sin(time*0.12): modulated noise \n",
    "\n",
    "\n",
    "    # scaling the data \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    # reshape(-1, 1): turns a 1D array into a 2D shape expected by MinMaxScaler, then .reshape(-1) flattens it again \n",
    "\n",
    "    # data splitting \n",
    "    samples = 2800 # 70% of 4000\n",
    "    train_data = amplitude[:samples]\n",
    "    test_data = amplitude[samples:]\n",
    "\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2d6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, batch_size): # extracts a mini-batch of input/label pairs from the training or test set (the source), starting at position i, and prepares the right shape for the model input\n",
    "    seq_len = min(batch_size, len(source) - 1 - i) # calculates how many items to include in the current batch - if you're near the end of the dataset, you may not have enough for a full batch so you'll take whatever is left\n",
    "    data = source[i:i+seq_len] # slices the dataset from i to i+seq_len\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a4a2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data): \n",
    "    # define the train function, takes in train_data, process it in batches to train the model for one epoch\n",
    "    model.train() # sets the mode to training mode \n",
    "    # this enables layers like Dropout and BatchNorm to behave correctly - as opposed to how they do during .eval() mode\n",
    "\n",
    "    total_loss = 0.0 # initialize loss counter \n",
    "    start_time = time.time() # starts timer at start of training  - used to calculate how long each batch takes\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        # enumerate gives: batch size index, the start index of the train_data\n",
    "\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        # extracts a batch of inputs starting at index i \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # clear old gradients from previous step \n",
    "\n",
    "        output = model(data)\n",
    "        # feeds input batch into the model to store the predictions\n",
    "\n",
    "        if calculate_loss_over_all_values: # uses entire sequence to calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "        else: # uses only the previous output window of timesteps to calculate loss\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "\n",
    "        loss.backward() # compute gradients of loss wrt the learnable parameters\n",
    "        # backward pass \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # clip gradients if norm exceeds 0.5 to prevent exploding gradients \n",
    "        optimizer.step() # apply gradients to update model weights \n",
    "        # this is where learning occurs \n",
    "\n",
    "        total_loss += loss.item() # add current loss to cummulative loss\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval \n",
    "            elapsed = time.time() - start_time \n",
    "            print ('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                    ' lr {:02.6f} | {:5.2f} ms | loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            # ppl = perplexity\n",
    "            total_loss = 0 \n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc04b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source, epoch): \n",
    "    # evaluates model on the dataset, data_source \n",
    "\n",
    "    eval_model.eval() # sets model to evaluatio model \n",
    "    total_loss = 0.0 # accumulated loss across test examples \n",
    "    test_result = torch.Tensor(0) # stores model outputs for plotting \n",
    "    truth = torch.Tensor(0) # ground truth values\n",
    "\n",
    "    #  torch.Tensor(0): creates empty 1D tensors \n",
    "\n",
    "    with torch.no_grad(): # opens the no-gradient context \n",
    "        # during evaluation, we don't need gradients \n",
    "\n",
    "        for i in range(0, len(data_source) - 1): # iterate over every sample of data_source except the last\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else: \n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "    \n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color = \"red\") # model predictions \n",
    "    pyplot.plot(truth[:500], color = \"blue\") # true values \n",
    "    pyplot.plot(test_result - truth, color = \"green\") # error\n",
    "    pyplot.grid(True, which = 'both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('grapho/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5df8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(eval_model, data_source, steps): \n",
    "    #  enables trained transformer to predict future time steps beyond the training window \n",
    "    eval_model.eval() # puts model in evaluation mode \n",
    "    total_loss = 0.0\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    _, data = get_batch(data_source, 0, 1) # obtain single sample from dataset to use as the initial input sequence \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps, 1): \n",
    "            input = torch.clone(data[-input_window:]) # create copy of last input_window timesteps from data\n",
    "            input[-output_window:] = 0 # zero out the last output_window values in the input. Simulates missing future steps for the model to predict \n",
    "            output = eval_model(data[-input_window:]) \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "        data = data.cpu().view(-1)\n",
    "\n",
    "        pyplot.plot(data, color = 'red') # full series (original and predicted)\n",
    "        pyplot.plot(data[:input_window], color = 'blue') # initial known input \n",
    "        pyplot.grid(True, which = 'both') \n",
    "        pyplot.axhline(y=0, color='k')\n",
    "        pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "        pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60eca47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source): \n",
    "    # defines function to evaluate the model's loss on the data_source dataset - this is called after each epoch for the validation set \n",
    "    eval_model.eval() # set model to evaluation mode\n",
    "    total_loss = 0.0 # initialize total loss\n",
    "    eval_batch_size = 1000 # set large batch size since we're not doing backprop \n",
    "    with torch.no_grad(): # being no-gradient context \n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "            else: \n",
    "                total_loss += len(data[0]) * criterion(output[-output_window:], targets[-output_window:]).cpu().item()\n",
    "            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_97120/1047780709.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_2634bauad6/croot/libtorch_1744642078920/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data() \n",
    "#  data preprocessing function \n",
    "\n",
    "model = TransAm().to(device)\n",
    "# instantiate transformer model \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# define loss function \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# set up optimizer (Adam with decoupled weight decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.98)\n",
    "# create learning rate scheduler \n",
    "# at every step size = 1, epoch, it multiples the LR by gamma=98 to encourage more stable convergence\n",
    "\n",
    "best_val_loss = float(\"inf\") # track the lowest validation loss\n",
    "best_model = None # best performing model \n",
    "\n",
    "for epoch in range(1, epochs + 1): # training loop from epoch = 1\n",
    "    epoch_start_time = time.time()  # start time for logging \n",
    "    train(train_data) # train model \n",
    "\n",
    "    if (epoch % 10 == 0): # every 10 epochs \n",
    "        val_loss = plot_and_loss(model, val_data, epoch) # compute val loss and plot model performance\n",
    "        predict_future(model, val_data, 200) # forecast 200 steps in future\n",
    "    else: \n",
    "        val_loss = evaluate(model, val_data)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
