{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b7955b",
   "metadata": {},
   "source": [
    "# Toy Problem - Transformer Application to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50377cbf",
   "metadata": {},
   "source": [
    "https://github.com/oliverguhr/transformer-time-series-prediction/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec12fa0",
   "metadata": {},
   "source": [
    "Description: This example is from the repo above. It contains 2 PyTorch models for a transformer-based time series prediction. The dataset is stored in ./daily-min-temperatures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import time \n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb50495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = source sequence length\n",
    "# T = target sequence length \n",
    "# N = batch size \n",
    "# E = feature number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43280924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 \n",
    "output_window = 5\n",
    "batch_size = 10 \n",
    "lr = 0.005\n",
    "epochs = 100\n",
    "device = torch.device(\"curda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8380cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # define a pytorch module that inherits nn.Module\n",
    "    \"\"\"\n",
    "    Positional encoding layer for transformer model. \n",
    "\n",
    "    Layer injects info about the relative or absolute position of the sequence, without adding learnable parameters. \n",
    "\n",
    "    Uses sin and cos fcns of different frequencies to encode position info. \n",
    "\n",
    "    Args: \n",
    "        d_model (int): dimension of the embedding space \n",
    "        max_len (int, optional): max sequence length supported. Default is 5000 \n",
    "\n",
    "    Attriutes: \n",
    "        pe (Tensor): Fixed positional encoding matrix of space (max_len, 1, d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # the \"Attributes\" part documents the instance variables inside the __init__\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000): # creates init method \n",
    "        super(PositionalEncoding, self).__init__() # super() lets us avoid referring to the base class explicitly\n",
    "        # https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods\n",
    "        pe = torch.zeros(max_len, d_model) # create empty matrix of shape max_len X d_model to hold the positional encodings\n",
    "        # row: position i.e. 0, 1, 2, \n",
    "        # column: dim of the embedding \n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # vector of positions [0, 1, 2, 3, ..., 4999]\n",
    "        # unsqueeze reshapes vector from [max_len,] to [max_len, 1] to enable broadcasting \n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # division term for the sin and cos fcns \n",
    "        # torch.arange(0, d_model, 2).float(): starts at 0, ends at d_model, step size = 2 \n",
    "        # -ln(10000)\n",
    "        # torch.exp = exp \n",
    "        # this comes from \"Attention is all you need\" paper where sin and cos fcns of different frequencies are used where each dimension of the positional encoding corresponds to a sine\n",
    "        # PE at dim i = PE_(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        # PE_(pos, 2i+1) = cos(pos/10000^(2i/d_model)) \n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # at even indices: sin(position * frequency)\n",
    "    \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # at odd indices: cos(position * frequency)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        # pe.unsqueeze(0) == adds a batch dimension so the shape becomes: [1, max_len, d_model]\n",
    "        # .transpose(0,1) == swaps the first and second dimensions such that the new shape is [max_len, 1, d_model]\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        # saving pe tensor. Tensor which is not a parameter, but should be part of the module's state. Used for tensors that need to be on the same device as the module. \n",
    "        # it's a fixed tensor stored with the model and moved to the GPU/CPU automatically \n",
    "        # this is NOT updated during backprop \n",
    "\n",
    "    def forward(self, x): # during the forward pass, x is the input with shape [sequence length, batch_size, d_model]\n",
    "        return x + self.pe[:x.size(0), :] # add the pe for the len of the input, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35741f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USE OF POSITIONAL ENCODING\n",
    "positional_encoder = PositionalEncoding(d_model = 512)\n",
    "sample_x = torch.randn(100, 32, 512) # tensor filled with random numbers from a standard normal distribution of shape [100, 32, 512]\n",
    "sample_encode = positional_encoder(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ff6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -1.1524, -0.2506],\n",
      "         [-0.5461, -0.6302, -0.6347],\n",
      "         [-1.0841, -0.1287, -0.6811]],\n",
      "\n",
      "        [[-0.5518,  1.5398,  1.0036],\n",
      "         [-0.4424,  0.2087,  0.0160],\n",
      "         [ 1.2970, -0.4725,  0.3149]],\n",
      "\n",
      "        [[-0.9780,  0.6038, -1.7178],\n",
      "         [-0.3399, -0.2990,  1.8007],\n",
      "         [ 0.6786,  0.5225, -0.0246]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_x[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63089d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -0.1524, -0.2506],\n",
      "         [-0.5461,  0.3698, -0.6347],\n",
      "         [-1.0841,  0.8713, -0.6811]],\n",
      "\n",
      "        [[ 0.2896,  2.0801,  1.8254],\n",
      "         [ 0.3990,  0.7490,  0.8379],\n",
      "         [ 2.1385,  0.0678,  1.1368]],\n",
      "\n",
      "        [[-0.0687,  0.1877, -0.7814],\n",
      "         [ 0.5694, -0.7151,  2.7371],\n",
      "         [ 1.5879,  0.1063,  0.9118]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_encode[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f201e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module): # defines a model class \n",
    "    def __init__(self, feature_size=250, num_layers=1, dropout=0.1): # initialize model \n",
    "        # feature size: input embedding dimension \n",
    "        # num_layers: transformer layers to stack \n",
    "        # dropout: dropout rate for regularization \n",
    "\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer' # string label \n",
    "        self.src_mask = None # used for sequence masking - important in autoregressive tasks like time-series and language modeling\n",
    "        self.pos_encoder = PositionalEncoding(feature_size) # giving position awareness to the input embeddings before processed by the transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout) # one transformer encoder layer with 10 heads \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers) # stack multiple copies of the encoder layer \n",
    "        self.decoder = nn.Linear(feature_size, 1) # maps the feature size to 1 (in time series forecasting, this maps one number per position)\n",
    "        self.init_weights() # calls init_weights methods to initialize the weights \n",
    "\n",
    "    def init_weights(self): \n",
    "        initrange = 0.1 # define a small numer to set range for random initialization \n",
    "        self.decoder.bias.data.zero_() # bias is the additive constant inside Linear layers. This overwrites all biases to 0 so that only weights matter. Random biases could introduce unwanted drift right at the start so we initialize at 0\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange) # initialize weights between -0.1 and 0.1. Smaller weights help stabilize training early on \n",
    "\n",
    "    def forward(self,src): # the forward pass for the model \n",
    "\n",
    "        # because this is a causal task (like time series or language modeling), we must prevent tokens from seeing the future, so we implement this mask to avoid attention to future tokens\n",
    "        # the mask is a square matrix of size (seq_length, seq_length)\n",
    "\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) !=len(src): # check if a new source mask needs to be created (or a new one)\n",
    "            # if self.src_mask is None then no mask was created yet\n",
    "            #if self.src_mask.size(0) != len(src), the input seq len has changed since the previous iteration therefore we need one of a new size \n",
    "\n",
    "            device = src.device # get the device where the input tensor lives to ensure the mask is on the same device \n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device) \n",
    "            # generate causal mask of len(src) such that the token can attend only to itself and earlier tokens\n",
    "            self.src_mask = mask \n",
    "\n",
    "        src = self.pos_encoder(src) # apply positional encoding to the input embeddings \n",
    "        output = self.transformer_encoder(src, self.src_mask) # pass position-encoded input into the stacked transformer encoder layers \n",
    "        # this is where mlti-head self-attention happens \n",
    "        # each token attends to the previous ones bc of the mask \n",
    "\n",
    "        output = self.decoder(output)\n",
    "        # apply Linear layer to every position \n",
    "        # turn the feature_size vector to a scalar \n",
    "        # this is the model's final prediction at each time step or token \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz): \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1) # create upper triangular matrix of ones, then trnaspose flips it to make lower triangle of 1s \n",
    "        mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0)) # convert 1s/0s into attention scores. mask == 0 ==> future positions, mask == 1 ==> self and past \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda7300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare sequence data for training model\n",
    "\n",
    "def create_inout_sequences(input_data, tw): \n",
    "    # function: slices a time series dataset into overlapping i/o sequences for training \n",
    "    # input_data: 1D array (the time series)\n",
    "    # tw: time window (the sequence length)\n",
    "    # output: list of (input sequence, label sequence) pairs \n",
    "\n",
    "    inout_seq = [] # preallocate memory to hold the (input, label) pairs \n",
    "    L = len(input_data) # stores the length of the sequence \n",
    "    for i in range(L-tw): # loop through the sequence to get all sliding windows of length tw\n",
    "        # stop at L-tw to ensure input_data[i:i+tw] stays within the length of the input_data \n",
    "\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window], output_window * [0])\n",
    "        # input_data[i:i+tw]: gives a window of length tw \n",
    "        # [:-output_window]: removes the last output_window values - therefore we're making the last value(s) we want the model to predict \n",
    "        # output_window * [0]: appends 0s at the end for the same length removed\n",
    "\n",
    "        train_label = input_data[i:i+tw]\n",
    "        # this is the ground truth - the full unmasked windwo including the parts zeroed out \n",
    "\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "        # saves tuple of the masked input and the ground label \n",
    "\n",
    "    return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates and prepares synthetic time series data for the transformer, including normalizaton and splitting into training and tests sets \n",
    "def get_data(): \n",
    "\n",
    "    # generate synthetic data of a continuous signal \n",
    "    time = np.arange(0, 400, 0.1) # len = 4000 \n",
    "    amplitude = np.sin(time) + np.sin(time * 0.05) + np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
    "    # np.sin(time): main wave \n",
    "    # np.sin(time*0.05): low-frequency drift\n",
    "    # np.sin(time*0.12): modulated noise \n",
    "\n",
    "    # scaling the data \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    # reshape(-1, 1): turns a 1D array into a 2D shape expected by MinMaxScaler, then .reshape(-1) flattens it again \n",
    "\n",
    "    # data splitting \n",
    "    samples = 2800 # 70% of 4000\n",
    "    train_data = amplitude[:samples]\n",
    "    test_data = amplitude[samples:]\n",
    "\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, batch_size): # extracts a mini-batch of input/label pairs from the training or test set (the source), starting at position i, and prepares the right shape for the model input\n",
    "    seq_len = min(batch_size, len(source) - 1 - i) # calculates how many items to include in the current batch - if you're near the end of the dataset, you may not have enough for a full batch so you'll take whatever is left\n",
    "    data = source[i:i+seq_len] # slices the dataset from i to i+seq_len\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data): \n",
    "    # define the train function, takes in train_data, process it in batches to train the model for one epoch\n",
    "    model.train() # sets the mode to training mode \n",
    "    # this enables layers like Dropout and BatchNorm to behave correctly - as opposed to how they do during .eval() mode\n",
    "\n",
    "    total_loss = 0.0 # initialize loss counter \n",
    "    start_time = time.time() # starts timer at start of training  - used to calculate how long each batch takes\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        # enumerate gives: batch size index, the start index of the train_data\n",
    "\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        # extracts a batch of inputs starting at index i \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # clear old gradients from previous step \n",
    "\n",
    "        output = model(data)\n",
    "        # feeds input batch into the model to store the predictions\n",
    "\n",
    "        if calculate_loss_over_all_values: # uses entire sequence to calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "        else: # uses only the previous output window of timesteps to calculate loss\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "\n",
    "        loss.backward() # compute gradients of loss wrt the learnable parameters\n",
    "        # backward pass \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # clip gradients if norm exceeds 0.5 to prevent exploding gradients \n",
    "        optimizer.step() # apply gradients to update model weights \n",
    "        # this is where learning occurs \n",
    "\n",
    "        total_loss += loss.item() # add current loss to cummulative loss\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval \n",
    "            elapsed = time.time() - start_time \n",
    "            print ('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                    ' lr {:02.6f} | {:5.2f} ms | loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            # ppl = perplexity\n",
    "            total_loss = 0 \n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc04b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source, epoch): \n",
    "    # evaluates model on the dataset, data_source \n",
    "\n",
    "    eval_model.eval() # sets model to evaluatio model \n",
    "    total_loss = 0.0 # accumulated loss across test examples \n",
    "    test_result = torch.Tensor(0) # stores model outputs for plotting \n",
    "    truth = torch.Tensor(0) # ground truth values\n",
    "\n",
    "    #  torch.Tensor(0): creates empty 1D tensors \n",
    "\n",
    "    with torch.no_grad(): # opens the no-gradient context \n",
    "        # during evaluation, we don't need gradients \n",
    "\n",
    "        for i in range(0, len(data_source) - 1): # iterate over every sample of data_source except the last\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else: \n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "    \n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color = \"red\") # model predictions \n",
    "    pyplot.plot(truth[:500], color = \"blue\") # true values \n",
    "    pyplot.plot(test_result - truth, color = \"green\") # error\n",
    "    pyplot.grid(True, which = 'both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(eval_model, data_source, steps): \n",
    "    #  enables trained transformer to predict future time steps beyond the training window \n",
    "    eval_model.eval() # puts model in evaluation mode \n",
    "    total_loss = 0.0\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    _, data = get_batch(data_source, 0, 1) # obtain single sample from dataset to use as the initial input sequence \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps, 1): \n",
    "            input = torch.clone(data[-input_window:]) # create copy of last input_window timesteps from data\n",
    "            input[-output_window:] = 0 # zero out the last output_window values in the input. Simulates missing future steps for the model to predict \n",
    "            output = eval_model(data[-input_window:]) \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "        data = data.cpu().view(-1)\n",
    "\n",
    "        pyplot.plot(data, color = 'red') # full series (original and predicted)\n",
    "        pyplot.plot(data[:input_window], color = 'blue') # initial known input \n",
    "        pyplot.grid(True, which = 'both') \n",
    "        pyplot.axhline(y=0, color='k')\n",
    "        pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "        pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eca47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source): \n",
    "    # defines function to evaluate the model's loss on the data_source dataset - this is called after each epoch for the validation set \n",
    "    eval_model.eval() # set model to evaluation mode\n",
    "    total_loss = 0.0 # initialize total loss\n",
    "    eval_batch_size = 1000 # set large batch size since we're not doing backprop \n",
    "    with torch.no_grad(): # being no-gradient context \n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "            else: \n",
    "                total_loss += len(data[0]) * criterion(output[-output_window:], targets[-output_window:]).cpu().item()\n",
    "            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45371",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = get_data() \n",
    "#  data preprocessing function \n",
    "\n",
    "model = TransAm().to(device)\n",
    "# instantiate transformer model \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# define loss function \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# set up optimizer (Adam with decoupled weight decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.98)\n",
    "# create learning rate scheduler \n",
    "# at every step size = 1, epoch, it multiples the LR by gamma=98 to encourage more stable convergence\n",
    "\n",
    "best_val_loss = float(\"inf\") # track the lowest validation loss\n",
    "best_model = None # best performing model \n",
    "\n",
    "for epoch in range(1, epochs + 1): # training loop from epoch = 1\n",
    "    epoch_start_time = time.time()  # start time for logging \n",
    "    train(train_data) # train model \n",
    "\n",
    "    if (epoch % 10 == 0): # every 10 epochs \n",
    "        val_loss = plot_and_loss(model, val_data, epoch) # compute val loss and plot model performance\n",
    "        predict_future(model, val_data, 200) # forecast 200 steps in future\n",
    "    else: \n",
    "        val_loss = evaluate(model, val_data)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ba111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f358c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:257: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "<>:257: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_13047/3125818542.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_2634bauad6/croot/libtorch_1744642078920/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.FloatTensor(inout_seq)\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_13047/3125818542.py:257: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "  if(epoch % 10 is 0):\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:536: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches | lr 0.005000 | 440.53 ms | loss 5.30309 | ppl   200.96\n",
      "| epoch   1 |   106/  269 batches | lr 0.005000 | 467.20 ms | loss 0.08589 | ppl     1.09\n",
      "| epoch   1 |   159/  269 batches | lr 0.005000 | 444.79 ms | loss 0.12388 | ppl     1.13\n",
      "| epoch   1 |   212/  269 batches | lr 0.005000 | 434.17 ms | loss 0.05552 | ppl     1.06\n",
      "| epoch   1 |   265/  269 batches | lr 0.005000 | 416.57 ms | loss 0.05051 | ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 120.31s | valid loss 0.11385 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches | lr 0.004802 | 460.81 ms | loss 0.06192 | ppl     1.06\n",
      "| epoch   2 |   106/  269 batches | lr 0.004802 | 446.98 ms | loss 0.07375 | ppl     1.08\n",
      "| epoch   2 |   159/  269 batches | lr 0.004802 | 446.87 ms | loss 0.07587 | ppl     1.08\n",
      "| epoch   2 |   212/  269 batches | lr 0.004802 | 439.00 ms | loss 0.04549 | ppl     1.05\n",
      "| epoch   2 |   265/  269 batches | lr 0.004802 | 348.44 ms | loss 0.05488 | ppl     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 117.26s | valid loss 0.18634 | valid ppl     1.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches | lr 0.004706 | 373.46 ms | loss 0.07197 | ppl     1.07\n",
      "| epoch   3 |   106/  269 batches | lr 0.004706 | 402.74 ms | loss 0.07621 | ppl     1.08\n",
      "| epoch   3 |   159/  269 batches | lr 0.004706 | 407.19 ms | loss 0.07397 | ppl     1.08\n",
      "| epoch   3 |   212/  269 batches | lr 0.004706 | 429.10 ms | loss 0.05464 | ppl     1.06\n",
      "| epoch   3 |   265/  269 batches | lr 0.004706 | 482.42 ms | loss 0.05032 | ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 114.48s | valid loss 0.21031 | valid ppl     1.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches | lr 0.004612 | 432.59 ms | loss 0.12585 | ppl     1.13\n",
      "| epoch   4 |   106/  269 batches | lr 0.004612 | 460.92 ms | loss 0.07821 | ppl     1.08\n",
      "| epoch   4 |   159/  269 batches | lr 0.004612 | 408.17 ms | loss 0.08614 | ppl     1.09\n",
      "| epoch   4 |   212/  269 batches | lr 0.004612 | 378.22 ms | loss 0.05783 | ppl     1.06\n",
      "| epoch   4 |   265/  269 batches | lr 0.004612 | 579.25 ms | loss 0.05248 | ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 123.04s | valid loss 0.24625 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches | lr 0.004520 | 364.08 ms | loss 0.11789 | ppl     1.13\n",
      "| epoch   5 |   106/  269 batches | lr 0.004520 | 452.60 ms | loss 0.08997 | ppl     1.09\n",
      "| epoch   5 |   159/  269 batches | lr 0.004520 | 371.98 ms | loss 0.08910 | ppl     1.09\n",
      "| epoch   5 |   212/  269 batches | lr 0.004520 | 413.24 ms | loss 0.06313 | ppl     1.07\n",
      "| epoch   5 |   265/  269 batches | lr 0.004520 | 380.56 ms | loss 0.07470 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 108.32s | valid loss 0.23986 | valid ppl     1.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches | lr 0.004429 | 413.42 ms | loss 0.12512 | ppl     1.13\n",
      "| epoch   6 |   106/  269 batches | lr 0.004429 | 354.88 ms | loss 0.13664 | ppl     1.15\n",
      "| epoch   6 |   159/  269 batches | lr 0.004429 | 402.99 ms | loss 0.09801 | ppl     1.10\n",
      "| epoch   6 |   212/  269 batches | lr 0.004429 | 409.67 ms | loss 0.07388 | ppl     1.08\n",
      "| epoch   6 |   265/  269 batches | lr 0.004429 | 427.38 ms | loss 0.07638 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 109.83s | valid loss 0.21701 | valid ppl     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches | lr 0.004341 | 418.74 ms | loss 0.12055 | ppl     1.13\n",
      "| epoch   7 |   106/  269 batches | lr 0.004341 | 521.54 ms | loss 0.13650 | ppl     1.15\n",
      "| epoch   7 |   159/  269 batches | lr 0.004341 | 415.74 ms | loss 0.12082 | ppl     1.13\n",
      "| epoch   7 |   212/  269 batches | lr 0.004341 | 392.77 ms | loss 0.10903 | ppl     1.12\n",
      "| epoch   7 |   265/  269 batches | lr 0.004341 | 434.98 ms | loss 0.10611 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 118.99s | valid loss 0.16737 | valid ppl     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    53/  269 batches | lr 0.004254 | 325.30 ms | loss 0.10427 | ppl     1.11\n",
      "| epoch   8 |   106/  269 batches | lr 0.004254 | 375.08 ms | loss 0.13569 | ppl     1.15\n",
      "| epoch   8 |   159/  269 batches | lr 0.004254 | 383.56 ms | loss 0.13525 | ppl     1.14\n",
      "| epoch   8 |   212/  269 batches | lr 0.004254 | 363.10 ms | loss 0.08551 | ppl     1.09\n",
      "| epoch   8 |   265/  269 batches | lr 0.004254 | 328.79 ms | loss 0.08482 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 97.32s | valid loss 0.28928 | valid ppl     1.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    53/  269 batches | lr 0.004169 | 408.39 ms | loss 0.13280 | ppl     1.14\n",
      "| epoch   9 |   106/  269 batches | lr 0.004169 | 383.55 ms | loss 0.13396 | ppl     1.14\n",
      "| epoch   9 |   159/  269 batches | lr 0.004169 | 387.77 ms | loss 0.11928 | ppl     1.13\n",
      "| epoch   9 |   212/  269 batches | lr 0.004169 | 430.71 ms | loss 0.08885 | ppl     1.09\n",
      "| epoch   9 |   265/  269 batches | lr 0.004169 | 376.73 ms | loss 0.08665 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 108.97s | valid loss 0.14766 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    53/  269 batches | lr 0.004085 | 417.15 ms | loss 0.12554 | ppl     1.13\n",
      "| epoch  10 |   106/  269 batches | lr 0.004085 | 428.08 ms | loss 0.14974 | ppl     1.16\n",
      "| epoch  10 |   159/  269 batches | lr 0.004085 | 386.90 ms | loss 0.11876 | ppl     1.13\n",
      "| epoch  10 |   212/  269 batches | lr 0.004085 | 412.50 ms | loss 0.10382 | ppl     1.11\n",
      "| epoch  10 |   265/  269 batches | lr 0.004085 | 424.31 ms | loss 0.10173 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 159.73s | valid loss 0.10973 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    53/  269 batches | lr 0.004004 | 337.56 ms | loss 0.13058 | ppl     1.14\n",
      "| epoch  11 |   106/  269 batches | lr 0.004004 | 394.34 ms | loss 0.12254 | ppl     1.13\n",
      "| epoch  11 |   159/  269 batches | lr 0.004004 | 427.36 ms | loss 0.10594 | ppl     1.11\n",
      "| epoch  11 |   212/  269 batches | lr 0.004004 | 343.74 ms | loss 0.10297 | ppl     1.11\n",
      "| epoch  11 |   265/  269 batches | lr 0.004004 | 368.66 ms | loss 0.11294 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 102.43s | valid loss 0.11089 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    53/  269 batches | lr 0.003924 | 391.94 ms | loss 0.10818 | ppl     1.11\n",
      "| epoch  12 |   106/  269 batches | lr 0.003924 | 366.50 ms | loss 0.16113 | ppl     1.17\n",
      "| epoch  12 |   159/  269 batches | lr 0.003924 | 412.56 ms | loss 0.16630 | ppl     1.18\n",
      "| epoch  12 |   212/  269 batches | lr 0.003924 | 418.82 ms | loss 0.10327 | ppl     1.11\n",
      "| epoch  12 |   265/  269 batches | lr 0.003924 | 561.93 ms | loss 0.10758 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 117.57s | valid loss 0.15332 | valid ppl     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    53/  269 batches | lr 0.003845 | 422.50 ms | loss 0.10686 | ppl     1.11\n",
      "| epoch  13 |   106/  269 batches | lr 0.003845 | 384.18 ms | loss 0.14213 | ppl     1.15\n",
      "| epoch  13 |   159/  269 batches | lr 0.003845 | 404.82 ms | loss 0.17680 | ppl     1.19\n",
      "| epoch  13 |   212/  269 batches | lr 0.003845 | 490.77 ms | loss 0.09161 | ppl     1.10\n",
      "| epoch  13 |   265/  269 batches | lr 0.003845 | 458.55 ms | loss 0.11305 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 118.80s | valid loss 0.10861 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    53/  269 batches | lr 0.003768 | 548.78 ms | loss 0.13731 | ppl     1.15\n",
      "| epoch  14 |   106/  269 batches | lr 0.003768 | 487.38 ms | loss 0.13851 | ppl     1.15\n",
      "| epoch  14 |   159/  269 batches | lr 0.003768 | 474.91 ms | loss 0.11026 | ppl     1.12\n",
      "| epoch  14 |   212/  269 batches | lr 0.003768 | 490.57 ms | loss 0.10283 | ppl     1.11\n",
      "| epoch  14 |   265/  269 batches | lr 0.003768 | 399.29 ms | loss 0.11334 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 131.54s | valid loss 0.10849 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    53/  269 batches | lr 0.003693 | 469.89 ms | loss 0.12264 | ppl     1.13\n",
      "| epoch  15 |   106/  269 batches | lr 0.003693 | 438.90 ms | loss 0.15014 | ppl     1.16\n",
      "| epoch  15 |   159/  269 batches | lr 0.003693 | 470.52 ms | loss 0.14210 | ppl     1.15\n",
      "| epoch  15 |   212/  269 batches | lr 0.003693 | 453.62 ms | loss 0.10474 | ppl     1.11\n",
      "| epoch  15 |   265/  269 batches | lr 0.003693 | 461.84 ms | loss 0.11148 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 125.21s | valid loss 0.13622 | valid ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    53/  269 batches | lr 0.003619 | 466.18 ms | loss 0.11649 | ppl     1.12\n",
      "| epoch  16 |   106/  269 batches | lr 0.003619 | 475.32 ms | loss 0.13322 | ppl     1.14\n",
      "| epoch  16 |   159/  269 batches | lr 0.003619 | 439.07 ms | loss 0.14714 | ppl     1.16\n",
      "| epoch  16 |   212/  269 batches | lr 0.003619 | 434.51 ms | loss 0.10506 | ppl     1.11\n",
      "| epoch  16 |   265/  269 batches | lr 0.003619 | 489.09 ms | loss 0.11610 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 125.85s | valid loss 0.10015 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    53/  269 batches | lr 0.003547 | 468.71 ms | loss 0.13726 | ppl     1.15\n",
      "| epoch  17 |   106/  269 batches | lr 0.003547 | 473.40 ms | loss 0.12992 | ppl     1.14\n",
      "| epoch  17 |   159/  269 batches | lr 0.003547 | 465.44 ms | loss 0.10777 | ppl     1.11\n",
      "| epoch  17 |   212/  269 batches | lr 0.003547 | 455.29 ms | loss 0.09893 | ppl     1.10\n",
      "| epoch  17 |   265/  269 batches | lr 0.003547 | 500.92 ms | loss 0.11235 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 129.18s | valid loss 0.10095 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    53/  269 batches | lr 0.003476 | 440.56 ms | loss 0.12092 | ppl     1.13\n",
      "| epoch  18 |   106/  269 batches | lr 0.003476 | 516.00 ms | loss 0.14818 | ppl     1.16\n",
      "| epoch  18 |   159/  269 batches | lr 0.003476 | 477.88 ms | loss 0.15657 | ppl     1.17\n",
      "| epoch  18 |   212/  269 batches | lr 0.003476 | 475.79 ms | loss 0.08786 | ppl     1.09\n",
      "| epoch  18 |   265/  269 batches | lr 0.003476 | 476.75 ms | loss 0.10825 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 130.07s | valid loss 0.10006 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    53/  269 batches | lr 0.003406 | 472.81 ms | loss 0.13276 | ppl     1.14\n",
      "| epoch  19 |   106/  269 batches | lr 0.003406 | 435.16 ms | loss 0.12913 | ppl     1.14\n",
      "| epoch  19 |   159/  269 batches | lr 0.003406 | 448.69 ms | loss 0.10613 | ppl     1.11\n",
      "| epoch  19 |   212/  269 batches | lr 0.003406 | 328.09 ms | loss 0.10059 | ppl     1.11\n",
      "| epoch  19 |   265/  269 batches | lr 0.003406 | 406.30 ms | loss 0.11161 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 113.96s | valid loss 0.11451 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    53/  269 batches | lr 0.003338 | 356.85 ms | loss 0.11655 | ppl     1.12\n",
      "| epoch  20 |   106/  269 batches | lr 0.003338 | 265.59 ms | loss 0.13496 | ppl     1.14\n",
      "| epoch  20 |   159/  269 batches | lr 0.003338 | 268.50 ms | loss 0.14796 | ppl     1.16\n",
      "| epoch  20 |   212/  269 batches | lr 0.003338 | 266.46 ms | loss 0.10200 | ppl     1.11\n",
      "| epoch  20 |   265/  269 batches | lr 0.003338 | 273.25 ms | loss 0.11318 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 98.20s | valid loss 0.10060 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    53/  269 batches | lr 0.003271 | 303.39 ms | loss 0.14019 | ppl     1.15\n",
      "| epoch  21 |   106/  269 batches | lr 0.003271 | 288.80 ms | loss 0.13152 | ppl     1.14\n",
      "| epoch  21 |   159/  269 batches | lr 0.003271 | 253.36 ms | loss 0.10717 | ppl     1.11\n",
      "| epoch  21 |   212/  269 batches | lr 0.003271 | 255.09 ms | loss 0.09865 | ppl     1.10\n",
      "| epoch  21 |   265/  269 batches | lr 0.003271 | 282.46 ms | loss 0.10917 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 76.00s | valid loss 0.10133 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    53/  269 batches | lr 0.003206 | 310.51 ms | loss 0.11631 | ppl     1.12\n",
      "| epoch  22 |   106/  269 batches | lr 0.003206 | 262.83 ms | loss 0.14255 | ppl     1.15\n",
      "| epoch  22 |   159/  269 batches | lr 0.003206 | 347.07 ms | loss 0.14838 | ppl     1.16\n",
      "| epoch  22 |   212/  269 batches | lr 0.003206 | 471.74 ms | loss 0.09002 | ppl     1.09\n",
      "| epoch  22 |   265/  269 batches | lr 0.003206 | 418.87 ms | loss 0.10646 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 99.16s | valid loss 0.10060 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    53/  269 batches | lr 0.003142 | 427.22 ms | loss 0.13554 | ppl     1.15\n",
      "| epoch  23 |   106/  269 batches | lr 0.003142 | 412.49 ms | loss 0.12442 | ppl     1.13\n",
      "| epoch  23 |   159/  269 batches | lr 0.003142 | 381.75 ms | loss 0.11486 | ppl     1.12\n",
      "| epoch  23 |   212/  269 batches | lr 0.003142 | 352.49 ms | loss 0.10082 | ppl     1.11\n",
      "| epoch  23 |   265/  269 batches | lr 0.003142 | 338.10 ms | loss 0.11132 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 104.11s | valid loss 0.10616 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    53/  269 batches | lr 0.003079 | 432.58 ms | loss 0.12176 | ppl     1.13\n",
      "| epoch  24 |   106/  269 batches | lr 0.003079 | 368.23 ms | loss 0.13275 | ppl     1.14\n",
      "| epoch  24 |   159/  269 batches | lr 0.003079 | 340.21 ms | loss 0.12528 | ppl     1.13\n",
      "| epoch  24 |   212/  269 batches | lr 0.003079 | 414.60 ms | loss 0.10197 | ppl     1.11\n",
      "| epoch  24 |   265/  269 batches | lr 0.003079 | 320.04 ms | loss 0.10773 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 102.14s | valid loss 0.10007 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    53/  269 batches | lr 0.003017 | 303.60 ms | loss 0.13059 | ppl     1.14\n",
      "| epoch  25 |   106/  269 batches | lr 0.003017 | 400.11 ms | loss 0.13709 | ppl     1.15\n",
      "| epoch  25 |   159/  269 batches | lr 0.003017 | 429.73 ms | loss 0.12179 | ppl     1.13\n",
      "| epoch  25 |   212/  269 batches | lr 0.003017 | 418.74 ms | loss 0.09986 | ppl     1.11\n",
      "| epoch  25 |   265/  269 batches | lr 0.003017 | 469.56 ms | loss 0.10755 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 110.35s | valid loss 0.10010 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    53/  269 batches | lr 0.002957 | 365.36 ms | loss 0.12959 | ppl     1.14\n",
      "| epoch  26 |   106/  269 batches | lr 0.002957 | 361.98 ms | loss 0.13841 | ppl     1.15\n",
      "| epoch  26 |   159/  269 batches | lr 0.002957 | 408.92 ms | loss 0.11969 | ppl     1.13\n",
      "| epoch  26 |   212/  269 batches | lr 0.002957 | 406.43 ms | loss 0.08486 | ppl     1.09\n",
      "| epoch  26 |   265/  269 batches | lr 0.002957 | 441.98 ms | loss 0.10328 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 109.97s | valid loss 0.10020 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    53/  269 batches | lr 0.002898 | 395.51 ms | loss 0.12977 | ppl     1.14\n",
      "| epoch  27 |   106/  269 batches | lr 0.002898 | 466.72 ms | loss 0.12066 | ppl     1.13\n",
      "| epoch  27 |   159/  269 batches | lr 0.002898 | 485.17 ms | loss 0.10481 | ppl     1.11\n",
      "| epoch  27 |   212/  269 batches | lr 0.002898 | 501.24 ms | loss 0.09417 | ppl     1.10\n",
      "| epoch  27 |   265/  269 batches | lr 0.002898 | 525.97 ms | loss 0.10424 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 128.99s | valid loss 0.10170 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    53/  269 batches | lr 0.002840 | 440.12 ms | loss 0.12838 | ppl     1.14\n",
      "| epoch  28 |   106/  269 batches | lr 0.002840 | 460.00 ms | loss 0.13669 | ppl     1.15\n",
      "| epoch  28 |   159/  269 batches | lr 0.002840 | 437.64 ms | loss 0.12989 | ppl     1.14\n",
      "| epoch  28 |   212/  269 batches | lr 0.002840 | 525.21 ms | loss 0.09256 | ppl     1.10\n",
      "| epoch  28 |   265/  269 batches | lr 0.002840 | 473.57 ms | loss 0.10951 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 126.96s | valid loss 0.10027 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    53/  269 batches | lr 0.002783 | 428.18 ms | loss 0.13416 | ppl     1.14\n",
      "| epoch  29 |   106/  269 batches | lr 0.002783 | 553.79 ms | loss 0.13345 | ppl     1.14\n",
      "| epoch  29 |   159/  269 batches | lr 0.002783 | 402.95 ms | loss 0.10710 | ppl     1.11\n",
      "| epoch  29 |   212/  269 batches | lr 0.002783 | 364.22 ms | loss 0.09509 | ppl     1.10\n",
      "| epoch  29 |   265/  269 batches | lr 0.002783 | 448.45 ms | loss 0.10047 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 119.45s | valid loss 0.10148 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    53/  269 batches | lr 0.002727 | 393.17 ms | loss 0.12574 | ppl     1.13\n",
      "| epoch  30 |   106/  269 batches | lr 0.002727 | 352.15 ms | loss 0.13306 | ppl     1.14\n",
      "| epoch  30 |   159/  269 batches | lr 0.002727 | 387.18 ms | loss 0.11683 | ppl     1.12\n",
      "| epoch  30 |   212/  269 batches | lr 0.002727 | 426.33 ms | loss 0.08651 | ppl     1.09\n",
      "| epoch  30 |   265/  269 batches | lr 0.002727 | 400.64 ms | loss 0.10340 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 143.65s | valid loss 0.10038 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    53/  269 batches | lr 0.002673 | 466.08 ms | loss 0.13721 | ppl     1.15\n",
      "| epoch  31 |   106/  269 batches | lr 0.002673 | 386.85 ms | loss 0.11863 | ppl     1.13\n",
      "| epoch  31 |   159/  269 batches | lr 0.002673 | 400.31 ms | loss 0.11337 | ppl     1.12\n",
      "| epoch  31 |   212/  269 batches | lr 0.002673 | 434.78 ms | loss 0.09419 | ppl     1.10\n",
      "| epoch  31 |   265/  269 batches | lr 0.002673 | 431.86 ms | loss 0.10321 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 115.91s | valid loss 0.10179 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    53/  269 batches | lr 0.002619 | 459.20 ms | loss 0.12183 | ppl     1.13\n",
      "| epoch  32 |   106/  269 batches | lr 0.002619 | 468.17 ms | loss 0.13241 | ppl     1.14\n",
      "| epoch  32 |   159/  269 batches | lr 0.002619 | 430.02 ms | loss 0.11928 | ppl     1.13\n",
      "| epoch  32 |   212/  269 batches | lr 0.002619 | 481.03 ms | loss 0.09278 | ppl     1.10\n",
      "| epoch  32 |   265/  269 batches | lr 0.002619 | 428.67 ms | loss 0.10371 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 123.25s | valid loss 0.10006 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    53/  269 batches | lr 0.002567 | 486.53 ms | loss 0.12928 | ppl     1.14\n",
      "| epoch  33 |   106/  269 batches | lr 0.002567 | 396.28 ms | loss 0.13295 | ppl     1.14\n",
      "| epoch  33 |   159/  269 batches | lr 0.002567 | 428.97 ms | loss 0.11582 | ppl     1.12\n",
      "| epoch  33 |   212/  269 batches | lr 0.002567 | 451.24 ms | loss 0.08739 | ppl     1.09\n",
      "| epoch  33 |   265/  269 batches | lr 0.002567 | 441.72 ms | loss 0.09957 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 120.60s | valid loss 0.10006 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    53/  269 batches | lr 0.002516 | 417.32 ms | loss 0.13174 | ppl     1.14\n",
      "| epoch  34 |   106/  269 batches | lr 0.002516 | 398.71 ms | loss 0.12450 | ppl     1.13\n",
      "| epoch  34 |   159/  269 batches | lr 0.002516 | 401.57 ms | loss 0.10842 | ppl     1.11\n",
      "| epoch  34 |   212/  269 batches | lr 0.002516 | 435.86 ms | loss 0.08707 | ppl     1.09\n",
      "| epoch  34 |   265/  269 batches | lr 0.002516 | 381.83 ms | loss 0.09735 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 110.68s | valid loss 0.10062 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    53/  269 batches | lr 0.002465 | 450.62 ms | loss 0.12355 | ppl     1.13\n",
      "| epoch  35 |   106/  269 batches | lr 0.002465 | 468.96 ms | loss 0.12816 | ppl     1.14\n",
      "| epoch  35 |   159/  269 batches | lr 0.002465 | 425.93 ms | loss 0.10990 | ppl     1.12\n",
      "| epoch  35 |   212/  269 batches | lr 0.002465 | 413.77 ms | loss 0.08180 | ppl     1.09\n",
      "| epoch  35 |   265/  269 batches | lr 0.002465 | 453.48 ms | loss 0.09656 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 120.66s | valid loss 0.10006 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    53/  269 batches | lr 0.002416 | 442.52 ms | loss 0.13070 | ppl     1.14\n",
      "| epoch  36 |   106/  269 batches | lr 0.002416 | 679.56 ms | loss 0.11990 | ppl     1.13\n",
      "| epoch  36 |   159/  269 batches | lr 0.002416 | 733.89 ms | loss 0.10696 | ppl     1.11\n",
      "| epoch  36 |   212/  269 batches | lr 0.002416 | 1714.80 ms | loss 0.08900 | ppl     1.09\n",
      "| epoch  36 |   265/  269 batches | lr 0.002416 | 379.93 ms | loss 0.09811 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 213.13s | valid loss 0.10045 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    53/  269 batches | lr 0.002368 | 676.74 ms | loss 0.12357 | ppl     1.13\n",
      "| epoch  37 |   106/  269 batches | lr 0.002368 | 474.73 ms | loss 0.13043 | ppl     1.14\n",
      "| epoch  37 |   159/  269 batches | lr 0.002368 | 425.49 ms | loss 0.10976 | ppl     1.12\n",
      "| epoch  37 |   212/  269 batches | lr 0.002368 | 386.04 ms | loss 0.08181 | ppl     1.09\n",
      "| epoch  37 |   265/  269 batches | lr 0.002368 | 429.32 ms | loss 0.09539 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 129.77s | valid loss 0.10008 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    53/  269 batches | lr 0.002320 | 430.66 ms | loss 0.12875 | ppl     1.14\n",
      "| epoch  38 |   106/  269 batches | lr 0.002320 | 344.91 ms | loss 0.12009 | ppl     1.13\n",
      "| epoch  38 |   159/  269 batches | lr 0.002320 | 384.03 ms | loss 0.10530 | ppl     1.11\n",
      "| epoch  38 |   212/  269 batches | lr 0.002320 | 425.48 ms | loss 0.08652 | ppl     1.09\n",
      "| epoch  38 |   265/  269 batches | lr 0.002320 | 358.31 ms | loss 0.09642 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 108.50s | valid loss 0.10051 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    53/  269 batches | lr 0.002274 | 519.33 ms | loss 0.12576 | ppl     1.13\n",
      "| epoch  39 |   106/  269 batches | lr 0.002274 | 1409.11 ms | loss 0.12541 | ppl     1.13\n",
      "| epoch  39 |   159/  269 batches | lr 0.002274 | 756.54 ms | loss 0.10916 | ppl     1.12\n",
      "| epoch  39 |   212/  269 batches | lr 0.002274 | 508.48 ms | loss 0.08319 | ppl     1.09\n",
      "| epoch  39 |   265/  269 batches | lr 0.002274 | 508.56 ms | loss 0.09719 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 200.38s | valid loss 0.10014 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    53/  269 batches | lr 0.002229 | 444.57 ms | loss 0.12951 | ppl     1.14\n",
      "| epoch  40 |   106/  269 batches | lr 0.002229 | 427.01 ms | loss 0.12321 | ppl     1.13\n",
      "| epoch  40 |   159/  269 batches | lr 0.002229 | 388.01 ms | loss 0.10464 | ppl     1.11\n",
      "| epoch  40 |   212/  269 batches | lr 0.002229 | 346.58 ms | loss 0.08525 | ppl     1.09\n",
      "| epoch  40 |   265/  269 batches | lr 0.002229 | 410.77 ms | loss 0.09466 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 149.09s | valid loss 0.10126 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    53/  269 batches | lr 0.002184 | 532.19 ms | loss 0.12048 | ppl     1.13\n",
      "| epoch  41 |   106/  269 batches | lr 0.002184 | 471.94 ms | loss 0.12633 | ppl     1.13\n",
      "| epoch  41 |   159/  269 batches | lr 0.002184 | 519.59 ms | loss 0.10688 | ppl     1.11\n",
      "| epoch  41 |   212/  269 batches | lr 0.002184 | 430.05 ms | loss 0.07945 | ppl     1.08\n",
      "| epoch  41 |   265/  269 batches | lr 0.002184 | 486.46 ms | loss 0.09289 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 132.88s | valid loss 0.10012 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    53/  269 batches | lr 0.002140 | 457.57 ms | loss 0.12930 | ppl     1.14\n",
      "| epoch  42 |   106/  269 batches | lr 0.002140 | 394.68 ms | loss 0.11339 | ppl     1.12\n",
      "| epoch  42 |   159/  269 batches | lr 0.002140 | 430.20 ms | loss 0.10583 | ppl     1.11\n",
      "| epoch  42 |   212/  269 batches | lr 0.002140 | 391.69 ms | loss 0.08350 | ppl     1.09\n",
      "| epoch  42 |   265/  269 batches | lr 0.002140 | 398.75 ms | loss 0.09570 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 114.86s | valid loss 0.10053 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    53/  269 batches | lr 0.002097 | 498.20 ms | loss 0.12693 | ppl     1.14\n",
      "| epoch  43 |   106/  269 batches | lr 0.002097 | 381.44 ms | loss 0.12208 | ppl     1.13\n",
      "| epoch  43 |   159/  269 batches | lr 0.002097 | 436.07 ms | loss 0.10561 | ppl     1.11\n",
      "| epoch  43 |   212/  269 batches | lr 0.002097 | 351.24 ms | loss 0.08133 | ppl     1.08\n",
      "| epoch  43 |   265/  269 batches | lr 0.002097 | 379.22 ms | loss 0.09362 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 111.64s | valid loss 0.10034 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    53/  269 batches | lr 0.002055 | 460.81 ms | loss 0.12853 | ppl     1.14\n",
      "| epoch  44 |   106/  269 batches | lr 0.002055 | 433.03 ms | loss 0.11394 | ppl     1.12\n",
      "| epoch  44 |   159/  269 batches | lr 0.002055 | 473.90 ms | loss 0.10363 | ppl     1.11\n",
      "| epoch  44 |   212/  269 batches | lr 0.002055 | 449.66 ms | loss 0.08304 | ppl     1.09\n",
      "| epoch  44 |   265/  269 batches | lr 0.002055 | 408.29 ms | loss 0.09623 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 121.54s | valid loss 0.10037 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    53/  269 batches | lr 0.002014 | 458.99 ms | loss 0.12742 | ppl     1.14\n",
      "| epoch  45 |   106/  269 batches | lr 0.002014 | 349.98 ms | loss 0.12262 | ppl     1.13\n",
      "| epoch  45 |   159/  269 batches | lr 0.002014 | 385.79 ms | loss 0.10291 | ppl     1.11\n",
      "| epoch  45 |   212/  269 batches | lr 0.002014 | 388.99 ms | loss 0.08189 | ppl     1.09\n",
      "| epoch  45 |   265/  269 batches | lr 0.002014 | 385.82 ms | loss 0.09335 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 107.66s | valid loss 0.10103 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    53/  269 batches | lr 0.001974 | 466.15 ms | loss 0.12479 | ppl     1.13\n",
      "| epoch  46 |   106/  269 batches | lr 0.001974 | 403.09 ms | loss 0.12061 | ppl     1.13\n",
      "| epoch  46 |   159/  269 batches | lr 0.001974 | 431.75 ms | loss 0.10241 | ppl     1.11\n",
      "| epoch  46 |   212/  269 batches | lr 0.001974 | 483.56 ms | loss 0.07716 | ppl     1.08\n",
      "| epoch  46 |   265/  269 batches | lr 0.001974 | 450.34 ms | loss 0.09148 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 124.33s | valid loss 0.10040 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    53/  269 batches | lr 0.001935 | 402.01 ms | loss 0.12562 | ppl     1.13\n",
      "| epoch  47 |   106/  269 batches | lr 0.001935 | 381.30 ms | loss 0.11372 | ppl     1.12\n",
      "| epoch  47 |   159/  269 batches | lr 0.001935 | 394.63 ms | loss 0.10193 | ppl     1.11\n",
      "| epoch  47 |   212/  269 batches | lr 0.001935 | 453.46 ms | loss 0.08100 | ppl     1.08\n",
      "| epoch  47 |   265/  269 batches | lr 0.001935 | 404.35 ms | loss 0.09235 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 111.13s | valid loss 0.10096 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    53/  269 batches | lr 0.001896 | 453.53 ms | loss 0.12293 | ppl     1.13\n",
      "| epoch  48 |   106/  269 batches | lr 0.001896 | 433.36 ms | loss 0.12174 | ppl     1.13\n",
      "| epoch  48 |   159/  269 batches | lr 0.001896 | 423.05 ms | loss 0.10348 | ppl     1.11\n",
      "| epoch  48 |   212/  269 batches | lr 0.001896 | 453.85 ms | loss 0.07844 | ppl     1.08\n",
      "| epoch  48 |   265/  269 batches | lr 0.001896 | 384.87 ms | loss 0.09195 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 117.27s | valid loss 0.10032 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    53/  269 batches | lr 0.001858 | 453.97 ms | loss 0.12490 | ppl     1.13\n",
      "| epoch  49 |   106/  269 batches | lr 0.001858 | 470.67 ms | loss 0.11155 | ppl     1.12\n",
      "| epoch  49 |   159/  269 batches | lr 0.001858 | 389.69 ms | loss 0.10253 | ppl     1.11\n",
      "| epoch  49 |   212/  269 batches | lr 0.001858 | 450.83 ms | loss 0.08352 | ppl     1.09\n",
      "| epoch  49 |   265/  269 batches | lr 0.001858 | 456.20 ms | loss 0.09355 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 121.04s | valid loss 0.10106 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    53/  269 batches | lr 0.001821 | 454.01 ms | loss 0.12377 | ppl     1.13\n",
      "| epoch  50 |   106/  269 batches | lr 0.001821 | 355.54 ms | loss 0.12031 | ppl     1.13\n",
      "| epoch  50 |   159/  269 batches | lr 0.001821 | 433.13 ms | loss 0.10276 | ppl     1.11\n",
      "| epoch  50 |   212/  269 batches | lr 0.001821 | 439.17 ms | loss 0.07772 | ppl     1.08\n",
      "| epoch  50 |   265/  269 batches | lr 0.001821 | 419.79 ms | loss 0.09206 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 151.72s | valid loss 0.10051 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    53/  269 batches | lr 0.001784 | 427.43 ms | loss 0.12691 | ppl     1.14\n",
      "| epoch  51 |   106/  269 batches | lr 0.001784 | 397.69 ms | loss 0.10819 | ppl     1.11\n",
      "| epoch  51 |   159/  269 batches | lr 0.001784 | 462.02 ms | loss 0.10118 | ppl     1.11\n",
      "| epoch  51 |   212/  269 batches | lr 0.001784 | 408.62 ms | loss 0.07924 | ppl     1.08\n",
      "| epoch  51 |   265/  269 batches | lr 0.001784 | 486.35 ms | loss 0.09164 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 118.92s | valid loss 0.10093 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    53/  269 batches | lr 0.001749 | 431.04 ms | loss 0.12401 | ppl     1.13\n",
      "| epoch  52 |   106/  269 batches | lr 0.001749 | 432.30 ms | loss 0.11117 | ppl     1.12\n",
      "| epoch  52 |   159/  269 batches | lr 0.001749 | 471.66 ms | loss 0.10000 | ppl     1.11\n",
      "| epoch  52 |   212/  269 batches | lr 0.001749 | 411.92 ms | loss 0.07884 | ppl     1.08\n",
      "| epoch  52 |   265/  269 batches | lr 0.001749 | 448.47 ms | loss 0.09148 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 119.71s | valid loss 0.10063 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    53/  269 batches | lr 0.001714 | 416.61 ms | loss 0.12658 | ppl     1.13\n",
      "| epoch  53 |   106/  269 batches | lr 0.001714 | 251.76 ms | loss 0.11266 | ppl     1.12\n",
      "| epoch  53 |   159/  269 batches | lr 0.001714 | 265.97 ms | loss 0.10157 | ppl     1.11\n",
      "| epoch  53 |   212/  269 batches | lr 0.001714 | 279.17 ms | loss 0.07891 | ppl     1.08\n",
      "| epoch  53 |   265/  269 batches | lr 0.001714 | 245.87 ms | loss 0.09170 | ppl     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 79.76s | valid loss 0.10061 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    53/  269 batches | lr 0.001679 | 286.09 ms | loss 0.12543 | ppl     1.13\n",
      "| epoch  54 |   106/  269 batches | lr 0.001679 | 240.73 ms | loss 0.11396 | ppl     1.12\n",
      "| epoch  54 |   159/  269 batches | lr 0.001679 | 298.22 ms | loss 0.10071 | ppl     1.11\n",
      "| epoch  54 |   212/  269 batches | lr 0.001679 | 250.28 ms | loss 0.07838 | ppl     1.08\n",
      "| epoch  54 |   265/  269 batches | lr 0.001679 | 249.83 ms | loss 0.09035 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 73.10s | valid loss 0.10076 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    53/  269 batches | lr 0.001646 | 269.14 ms | loss 0.12525 | ppl     1.13\n",
      "| epoch  55 |   106/  269 batches | lr 0.001646 | 284.92 ms | loss 0.11087 | ppl     1.12\n",
      "| epoch  55 |   159/  269 batches | lr 0.001646 | 256.00 ms | loss 0.10066 | ppl     1.11\n",
      "| epoch  55 |   212/  269 batches | lr 0.001646 | 259.59 ms | loss 0.07711 | ppl     1.08\n",
      "| epoch  55 |   265/  269 batches | lr 0.001646 | 250.53 ms | loss 0.08988 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 72.65s | valid loss 0.10078 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    53/  269 batches | lr 0.001613 | 292.66 ms | loss 0.12455 | ppl     1.13\n",
      "| epoch  56 |   106/  269 batches | lr 0.001613 | 265.39 ms | loss 0.10857 | ppl     1.11\n",
      "| epoch  56 |   159/  269 batches | lr 0.001613 | 263.13 ms | loss 0.10028 | ppl     1.11\n",
      "| epoch  56 |   212/  269 batches | lr 0.001613 | 241.26 ms | loss 0.07706 | ppl     1.08\n",
      "| epoch  56 |   265/  269 batches | lr 0.001613 | 291.21 ms | loss 0.09040 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 74.25s | valid loss 0.10067 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    53/  269 batches | lr 0.001581 | 258.77 ms | loss 0.12343 | ppl     1.13\n",
      "| epoch  57 |   106/  269 batches | lr 0.001581 | 258.47 ms | loss 0.11085 | ppl     1.12\n",
      "| epoch  57 |   159/  269 batches | lr 0.001581 | 251.38 ms | loss 0.09994 | ppl     1.11\n",
      "| epoch  57 |   212/  269 batches | lr 0.001581 | 284.91 ms | loss 0.07851 | ppl     1.08\n",
      "| epoch  57 |   265/  269 batches | lr 0.001581 | 259.17 ms | loss 0.09042 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 72.08s | valid loss 0.10123 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    53/  269 batches | lr 0.001549 | 281.88 ms | loss 0.12252 | ppl     1.13\n",
      "| epoch  58 |   106/  269 batches | lr 0.001549 | 299.04 ms | loss 0.11554 | ppl     1.12\n",
      "| epoch  58 |   159/  269 batches | lr 0.001549 | 351.53 ms | loss 0.10042 | ppl     1.11\n",
      "| epoch  58 |   212/  269 batches | lr 0.001549 | 298.88 ms | loss 0.07568 | ppl     1.08\n",
      "| epoch  58 |   265/  269 batches | lr 0.001549 | 319.09 ms | loss 0.08901 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 85.47s | valid loss 0.10072 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    53/  269 batches | lr 0.001518 | 349.44 ms | loss 0.12354 | ppl     1.13\n",
      "| epoch  59 |   106/  269 batches | lr 0.001518 | 321.20 ms | loss 0.10638 | ppl     1.11\n",
      "| epoch  59 |   159/  269 batches | lr 0.001518 | 270.00 ms | loss 0.09905 | ppl     1.10\n",
      "| epoch  59 |   212/  269 batches | lr 0.001518 | 360.14 ms | loss 0.07595 | ppl     1.08\n",
      "| epoch  59 |   265/  269 batches | lr 0.001518 | 284.70 ms | loss 0.08918 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 87.14s | valid loss 0.10118 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    53/  269 batches | lr 0.001488 | 346.08 ms | loss 0.12256 | ppl     1.13\n",
      "| epoch  60 |   106/  269 batches | lr 0.001488 | 315.96 ms | loss 0.11043 | ppl     1.12\n",
      "| epoch  60 |   159/  269 batches | lr 0.001488 | 271.29 ms | loss 0.09936 | ppl     1.10\n",
      "| epoch  60 |   212/  269 batches | lr 0.001488 | 284.91 ms | loss 0.07678 | ppl     1.08\n",
      "| epoch  60 |   265/  269 batches | lr 0.001488 | 298.14 ms | loss 0.08953 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 103.53s | valid loss 0.10152 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |    53/  269 batches | lr 0.001458 | 379.86 ms | loss 0.12229 | ppl     1.13\n",
      "| epoch  61 |   106/  269 batches | lr 0.001458 | 304.04 ms | loss 0.11432 | ppl     1.12\n",
      "| epoch  61 |   159/  269 batches | lr 0.001458 | 291.33 ms | loss 0.10038 | ppl     1.11\n",
      "| epoch  61 |   212/  269 batches | lr 0.001458 | 275.58 ms | loss 0.07631 | ppl     1.08\n",
      "| epoch  61 |   265/  269 batches | lr 0.001458 | 270.64 ms | loss 0.08894 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 83.26s | valid loss 0.10085 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    53/  269 batches | lr 0.001429 | 308.45 ms | loss 0.12249 | ppl     1.13\n",
      "| epoch  62 |   106/  269 batches | lr 0.001429 | 308.01 ms | loss 0.10726 | ppl     1.11\n",
      "| epoch  62 |   159/  269 batches | lr 0.001429 | 305.06 ms | loss 0.09888 | ppl     1.10\n",
      "| epoch  62 |   212/  269 batches | lr 0.001429 | 286.82 ms | loss 0.07584 | ppl     1.08\n",
      "| epoch  62 |   265/  269 batches | lr 0.001429 | 333.16 ms | loss 0.08887 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 85.34s | valid loss 0.10115 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    53/  269 batches | lr 0.001400 | 403.00 ms | loss 0.12212 | ppl     1.13\n",
      "| epoch  63 |   106/  269 batches | lr 0.001400 | 333.97 ms | loss 0.10931 | ppl     1.12\n",
      "| epoch  63 |   159/  269 batches | lr 0.001400 | 278.31 ms | loss 0.09911 | ppl     1.10\n",
      "| epoch  63 |   212/  269 batches | lr 0.001400 | 275.25 ms | loss 0.07544 | ppl     1.08\n",
      "| epoch  63 |   265/  269 batches | lr 0.001400 | 311.46 ms | loss 0.08866 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 88.17s | valid loss 0.10100 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    53/  269 batches | lr 0.001372 | 384.50 ms | loss 0.12055 | ppl     1.13\n",
      "| epoch  64 |   106/  269 batches | lr 0.001372 | 293.18 ms | loss 0.10640 | ppl     1.11\n",
      "| epoch  64 |   159/  269 batches | lr 0.001372 | 369.88 ms | loss 0.09844 | ppl     1.10\n",
      "| epoch  64 |   212/  269 batches | lr 0.001372 | 337.52 ms | loss 0.07556 | ppl     1.08\n",
      "| epoch  64 |   265/  269 batches | lr 0.001372 | 285.82 ms | loss 0.08898 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 91.54s | valid loss 0.10119 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    53/  269 batches | lr 0.001345 | 303.33 ms | loss 0.12161 | ppl     1.13\n",
      "| epoch  65 |   106/  269 batches | lr 0.001345 | 279.24 ms | loss 0.10914 | ppl     1.12\n",
      "| epoch  65 |   159/  269 batches | lr 0.001345 | 281.09 ms | loss 0.09877 | ppl     1.10\n",
      "| epoch  65 |   212/  269 batches | lr 0.001345 | 279.79 ms | loss 0.07584 | ppl     1.08\n",
      "| epoch  65 |   265/  269 batches | lr 0.001345 | 246.80 ms | loss 0.08867 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 76.20s | valid loss 0.10135 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    53/  269 batches | lr 0.001318 | 259.36 ms | loss 0.12194 | ppl     1.13\n",
      "| epoch  66 |   106/  269 batches | lr 0.001318 | 245.63 ms | loss 0.10774 | ppl     1.11\n",
      "| epoch  66 |   159/  269 batches | lr 0.001318 | 281.35 ms | loss 0.09845 | ppl     1.10\n",
      "| epoch  66 |   212/  269 batches | lr 0.001318 | 291.29 ms | loss 0.07508 | ppl     1.08\n",
      "| epoch  66 |   265/  269 batches | lr 0.001318 | 257.89 ms | loss 0.08874 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 73.39s | valid loss 0.10109 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    53/  269 batches | lr 0.001292 | 374.54 ms | loss 0.12144 | ppl     1.13\n",
      "| epoch  67 |   106/  269 batches | lr 0.001292 | 290.10 ms | loss 0.10706 | ppl     1.11\n",
      "| epoch  67 |   159/  269 batches | lr 0.001292 | 269.03 ms | loss 0.09864 | ppl     1.10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "input_window = 100\n",
    "output_window = 5\n",
    "batch_size = 10 # batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "       \n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "def get_data():\n",
    "    # time        = np.arange(0, 400, 0.1)\n",
    "    # amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
    "    \n",
    "    from pandas import read_csv\n",
    "    series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True)\n",
    "    # series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    # amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    \n",
    "    sampels = 2800\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment.. \n",
    "    train_sequence = create_inout_sequences(train_data,input_window)\n",
    "    train_sequence = train_sequence[:-output_window] #todo: fix hack?\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,input_window)\n",
    "    test_data = test_data[:-output_window] #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device),test_data.to(device)\n",
    "\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i,batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)        \n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "    pyplot.plot(test_result-truth,color=\"green\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "    \n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.close()\n",
    "        \n",
    "# entweder ist hier ein fehler im loss oder in der train methode, aber die ergebnisse sind unterschiedlich \n",
    "# auch zu denen der predict_future\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).cpu().item()            \n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "train_data, val_data = get_data()\n",
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    \n",
    "    if(epoch % 10 is 0):\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() \n",
    "\n",
    "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number) \n",
    "#out = model(src)\n",
    "#\n",
    "#print(out)\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f8c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
