{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b7955b",
   "metadata": {},
   "source": [
    "# Toy Problem - Transformer Application to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50377cbf",
   "metadata": {},
   "source": [
    "https://github.com/oliverguhr/transformer-time-series-prediction/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec12fa0",
   "metadata": {},
   "source": [
    "Description: This example is from the repo above. It contains 2 PyTorch models for a transformer-based time series prediction. The dataset is stored in ./daily-min-temperatures.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4c5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import time \n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb50495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cf8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = source sequence length\n",
    "# T = target sequence length \n",
    "# N = batch size \n",
    "# E = feature number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43280924",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 \n",
    "output_window = 5\n",
    "batch_size = 10 \n",
    "lr = 0.005\n",
    "epochs = 100\n",
    "device = torch.device(\"curda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8380cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # define a pytorch module that inherits nn.Module\n",
    "    \"\"\"\n",
    "    Positional encoding layer for transformer model. \n",
    "\n",
    "    Layer injects info about the relative or absolute position of the sequence, without adding learnable parameters. \n",
    "\n",
    "    Uses sin and cos fcns of different frequencies to encode position info. \n",
    "\n",
    "    Args: \n",
    "        d_model (int): dimension of the embedding space \n",
    "        max_len (int, optional): max sequence length supported. Default is 5000 \n",
    "\n",
    "    Attriutes: \n",
    "        pe (Tensor): Fixed positional encoding matrix of space (max_len, 1, d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # the \"Attributes\" part documents the instance variables inside the __init__\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000): # creates init method \n",
    "        super(PositionalEncoding, self).__init__() # super() lets us avoid referring to the base class explicitly\n",
    "        # https://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods\n",
    "        pe = torch.zeros(max_len, d_model) # create empty matrix of shape max_len X d_model to hold the positional encodings\n",
    "        # row: position i.e. 0, 1, 2, \n",
    "        # column: dim of the embedding \n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # vector of positions [0, 1, 2, 3, ..., 4999]\n",
    "        # unsqueeze reshapes vector from [max_len,] to [max_len, 1] to enable broadcasting \n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # division term for the sin and cos fcns \n",
    "        # torch.arange(0, d_model, 2).float(): starts at 0, ends at d_model, step size = 2 \n",
    "        # -ln(10000)\n",
    "        # torch.exp = exp \n",
    "        # this comes from \"Attention is all you need\" paper where sin and cos fcns of different frequencies are used where each dimension of the positional encoding corresponds to a sine\n",
    "        # PE at dim i = PE_(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        # PE_(pos, 2i+1) = cos(pos/10000^(2i/d_model)) \n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # at even indices: sin(position * frequency)\n",
    "    \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # at odd indices: cos(position * frequency)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        # pe.unsqueeze(0) == adds a batch dimension so the shape becomes: [1, max_len, d_model]\n",
    "        # .transpose(0,1) == swaps the first and second dimensions such that the new shape is [max_len, 1, d_model]\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        # saving pe tensor. Tensor which is not a parameter, but should be part of the module's state. Used for tensors that need to be on the same device as the module. \n",
    "        # it's a fixed tensor stored with the model and moved to the GPU/CPU automatically \n",
    "        # this is NOT updated during backprop \n",
    "\n",
    "    def forward(self, x): # during the forward pass, x is the input with shape [sequence length, batch_size, d_model]\n",
    "        return x + self.pe[:x.size(0), :] # add the pe for the len of the input, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35741f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USE OF POSITIONAL ENCODING\n",
    "positional_encoder = PositionalEncoding(d_model = 512)\n",
    "sample_x = torch.randn(100, 32, 512) # tensor filled with random numbers from a standard normal distribution of shape [100, 32, 512]\n",
    "sample_encode = positional_encoder(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ff6993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -1.1524, -0.2506],\n",
      "         [-0.5461, -0.6302, -0.6347],\n",
      "         [-1.0841, -0.1287, -0.6811]],\n",
      "\n",
      "        [[-0.5518,  1.5398,  1.0036],\n",
      "         [-0.4424,  0.2087,  0.0160],\n",
      "         [ 1.2970, -0.4725,  0.3149]],\n",
      "\n",
      "        [[-0.9780,  0.6038, -1.7178],\n",
      "         [-0.3399, -0.2990,  1.8007],\n",
      "         [ 0.6786,  0.5225, -0.0246]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_x[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63089d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1258, -0.1524, -0.2506],\n",
      "         [-0.5461,  0.3698, -0.6347],\n",
      "         [-1.0841,  0.8713, -0.6811]],\n",
      "\n",
      "        [[ 0.2896,  2.0801,  1.8254],\n",
      "         [ 0.3990,  0.7490,  0.8379],\n",
      "         [ 2.1385,  0.0678,  1.1368]],\n",
      "\n",
      "        [[-0.0687,  0.1877, -0.7814],\n",
      "         [ 0.5694, -0.7151,  2.7371],\n",
      "         [ 1.5879,  0.1063,  0.9118]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_encode[0:3, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f201e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module): # defines a model class \n",
    "    def __init__(self, feature_size=250, num_layers=1, dropout=0.1): # initialize model \n",
    "        # feature size: input embedding dimension \n",
    "        # num_layers: transformer layers to stack \n",
    "        # dropout: dropout rate for regularization \n",
    "\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer' # string label \n",
    "        self.src_mask = None # used for sequence masking - important in autoregressive tasks like time-series and language modeling\n",
    "        self.pos_encoder = PositionalEncoding(feature_size) # giving position awareness to the input embeddings before processed by the transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout) # one transformer encoder layer with 10 heads \n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers) # stack multiple copies of the encoder layer \n",
    "        self.decoder = nn.Linear(feature_size, 1) # maps the feature size to 1 (in time series forecasting, this maps one number per position)\n",
    "        self.init_weights() # calls init_weights methods to initialize the weights \n",
    "\n",
    "    def init_weights(self): \n",
    "        initrange = 0.1 # define a small numer to set range for random initialization \n",
    "        self.decoder.bias.data.zero_() # bias is the additive constant inside Linear layers. This overwrites all biases to 0 so that only weights matter. Random biases could introduce unwanted drift right at the start so we initialize at 0\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange) # initialize weights between -0.1 and 0.1. Smaller weights help stabilize training early on \n",
    "\n",
    "    def forward(self,src): # the forward pass for the model \n",
    "\n",
    "        # because this is a causal task (like time series or language modeling), we must prevent tokens from seeing the future, so we implement this mask to avoid attention to future tokens\n",
    "        # the mask is a square matrix of size (seq_length, seq_length)\n",
    "\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) !=len(src): # check if a new source mask needs to be created (or a new one)\n",
    "            # if self.src_mask is None then no mask was created yet\n",
    "            #if self.src_mask.size(0) != len(src), the input seq len has changed since the previous iteration therefore we need one of a new size \n",
    "\n",
    "            device = src.device # get the device where the input tensor lives to ensure the mask is on the same device \n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device) \n",
    "            # generate causal mask of len(src) such that the token can attend only to itself and earlier tokens\n",
    "            self.src_mask = mask \n",
    "\n",
    "        src = self.pos_encoder(src) # apply positional encoding to the input embeddings \n",
    "        output = self.transformer_encoder(src, self.src_mask) # pass position-encoded input into the stacked transformer encoder layers \n",
    "        # this is where mlti-head self-attention happens \n",
    "        # each token attends to the previous ones bc of the mask \n",
    "\n",
    "        output = self.decoder(output)\n",
    "        # apply Linear layer to every position \n",
    "        # turn the feature_size vector to a scalar \n",
    "        # this is the model's final prediction at each time step or token \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz): \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0,1) # create upper triangular matrix of ones, then trnaspose flips it to make lower triangle of 1s \n",
    "        mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask==1, float(0.0)) # convert 1s/0s into attention scores. mask == 0 ==> future positions, mask == 1 ==> self and past \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda7300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare sequence data for training model\n",
    "\n",
    "def create_inout_sequences(input_data, tw): \n",
    "    # function: slices a time series dataset into overlapping i/o sequences for training \n",
    "    # input_data: 1D array (the time series)\n",
    "    # tw: time window (the sequence length)\n",
    "    # output: list of (input sequence, label sequence) pairs \n",
    "\n",
    "    inout_seq = [] # preallocate memory to hold the (input, label) pairs \n",
    "    L = len(input_data) # stores the length of the sequence \n",
    "    for i in range(L-tw): # loop through the sequence to get all sliding windows of length tw\n",
    "        # stop at L-tw to ensure input_data[i:i+tw] stays within the length of the input_data \n",
    "\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window], output_window * [0])\n",
    "        # input_data[i:i+tw]: gives a window of length tw \n",
    "        # [:-output_window]: removes the last output_window values - therefore we're making the last value(s) we want the model to predict \n",
    "        # output_window * [0]: appends 0s at the end for the same length removed\n",
    "\n",
    "        train_label = input_data[i:i+tw]\n",
    "        # this is the ground truth - the full unmasked windwo including the parts zeroed out \n",
    "\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "        # saves tuple of the masked input and the ground label \n",
    "\n",
    "    return torch.FloatTensor(inout_seq) # convert the tuple into a PyTorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates and prepares synthetic time series data for the transformer, including normalizaton and splitting into training and tests sets \n",
    "def get_data(): \n",
    "\n",
    "    # generate synthetic data of a continuous signal \n",
    "    time = np.arange(0, 400, 0.1) # len = 4000 \n",
    "    amplitude = np.sin(time) + np.sin(time * 0.05) + np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
    "    # np.sin(time): main wave \n",
    "    # np.sin(time*0.05): low-frequency drift\n",
    "    # np.sin(time*0.12): modulated noise \n",
    "\n",
    "    # scaling the data \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    # reshape(-1, 1): turns a 1D array into a 2D shape expected by MinMaxScaler, then .reshape(-1) flattens it again \n",
    "\n",
    "    # data splitting \n",
    "    samples = 2800 # 70% of 4000\n",
    "    train_data = amplitude[:samples]\n",
    "    test_data = amplitude[samples:]\n",
    "\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, batch_size): # extracts a mini-batch of input/label pairs from the training or test set (the source), starting at position i, and prepares the right shape for the model input\n",
    "    seq_len = min(batch_size, len(source) - 1 - i) # calculates how many items to include in the current batch - if you're near the end of the dataset, you may not have enough for a full batch so you'll take whatever is left\n",
    "    data = source[i:i+seq_len] # slices the dataset from i to i+seq_len\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data): \n",
    "    # define the train function, takes in train_data, process it in batches to train the model for one epoch\n",
    "    model.train() # sets the mode to training mode \n",
    "    # this enables layers like Dropout and BatchNorm to behave correctly - as opposed to how they do during .eval() mode\n",
    "\n",
    "    total_loss = 0.0 # initialize loss counter \n",
    "    start_time = time.time() # starts timer at start of training  - used to calculate how long each batch takes\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        # enumerate gives: batch size index, the start index of the train_data\n",
    "\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        # extracts a batch of inputs starting at index i \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # clear old gradients from previous step \n",
    "\n",
    "        output = model(data)\n",
    "        # feeds input batch into the model to store the predictions\n",
    "\n",
    "        if calculate_loss_over_all_values: # uses entire sequence to calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "        else: # uses only the previous output window of timesteps to calculate loss\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "\n",
    "        loss.backward() # compute gradients of loss wrt the learnable parameters\n",
    "        # backward pass \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # clip gradients if norm exceeds 0.5 to prevent exploding gradients \n",
    "        optimizer.step() # apply gradients to update model weights \n",
    "        # this is where learning occurs \n",
    "\n",
    "        total_loss += loss.item() # add current loss to cummulative loss\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval \n",
    "            elapsed = time.time() - start_time \n",
    "            print ('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                    ' lr {:02.6f} | {:5.2f} ms | loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            # ppl = perplexity\n",
    "            total_loss = 0 \n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc04b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source, epoch): \n",
    "    # evaluates model on the dataset, data_source \n",
    "\n",
    "    eval_model.eval() # sets model to evaluatio model \n",
    "    total_loss = 0.0 # accumulated loss across test examples \n",
    "    test_result = torch.Tensor(0) # stores model outputs for plotting \n",
    "    truth = torch.Tensor(0) # ground truth values\n",
    "\n",
    "    #  torch.Tensor(0): creates empty 1D tensors \n",
    "\n",
    "    with torch.no_grad(): # opens the no-gradient context \n",
    "        # during evaluation, we don't need gradients \n",
    "\n",
    "        for i in range(0, len(data_source) - 1): # iterate over every sample of data_source except the last\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else: \n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "    \n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color = \"red\") # model predictions \n",
    "    pyplot.plot(truth[:500], color = \"blue\") # true values \n",
    "    pyplot.plot(test_result - truth, color = \"green\") # error\n",
    "    pyplot.grid(True, which = 'both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(eval_model, data_source, steps): \n",
    "    #  enables trained transformer to predict future time steps beyond the training window \n",
    "    eval_model.eval() # puts model in evaluation mode \n",
    "    total_loss = 0.0\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    _, data = get_batch(data_source, 0, 1) # obtain single sample from dataset to use as the initial input sequence \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps, 1): \n",
    "            input = torch.clone(data[-input_window:]) # create copy of last input_window timesteps from data\n",
    "            input[-output_window:] = 0 # zero out the last output_window values in the input. Simulates missing future steps for the model to predict \n",
    "            output = eval_model(data[-input_window:]) \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "        data = data.cpu().view(-1)\n",
    "\n",
    "        pyplot.plot(data, color = 'red') # full series (original and predicted)\n",
    "        pyplot.plot(data[:input_window], color = 'blue') # initial known input \n",
    "        pyplot.grid(True, which = 'both') \n",
    "        pyplot.axhline(y=0, color='k')\n",
    "        pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "        pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eca47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source): \n",
    "    # defines function to evaluate the model's loss on the data_source dataset - this is called after each epoch for the validation set \n",
    "    eval_model.eval() # set model to evaluation mode\n",
    "    total_loss = 0.0 # initialize total loss\n",
    "    eval_batch_size = 1000 # set large batch size since we're not doing backprop \n",
    "    with torch.no_grad(): # being no-gradient context \n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "\n",
    "            if calculate_loss_over_all_values: \n",
    "                total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "            else: \n",
    "                total_loss += len(data[0]) * criterion(output[-output_window:], targets[-output_window:]).cpu().item()\n",
    "            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45371",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = get_data() \n",
    "#  data preprocessing function \n",
    "\n",
    "model = TransAm().to(device)\n",
    "# instantiate transformer model \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# define loss function \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# set up optimizer (Adam with decoupled weight decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.98)\n",
    "# create learning rate scheduler \n",
    "# at every step size = 1, epoch, it multiples the LR by gamma=98 to encourage more stable convergence\n",
    "\n",
    "best_val_loss = float(\"inf\") # track the lowest validation loss\n",
    "best_model = None # best performing model \n",
    "\n",
    "for epoch in range(1, epochs + 1): # training loop from epoch = 1\n",
    "    epoch_start_time = time.time()  # start time for logging \n",
    "    train(train_data) # train model \n",
    "\n",
    "    if (epoch % 10 == 0): # every 10 epochs \n",
    "        val_loss = plot_and_loss(model, val_data, epoch) # compute val loss and plot model performance\n",
    "        predict_future(model, val_data, 200) # forecast 200 steps in future\n",
    "    else: \n",
    "        val_loss = evaluate(model, val_data)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ba111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f358c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50dd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:257: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "<>:257: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_13047/3125818542.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_2634bauad6/croot/libtorch_1744642078920/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.FloatTensor(inout_seq)\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/var/folders/jm/d49dqhd91j9g3f2wf9zhq0z80000gn/T/ipykernel_13047/3125818542.py:257: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "  if(epoch % 10 is 0):\n",
      "/Users/marilyn/anaconda3/envs/toy_transformer/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:536: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches | lr 0.005000 | 440.53 ms | loss 5.30309 | ppl   200.96\n",
      "| epoch   1 |   106/  269 batches | lr 0.005000 | 467.20 ms | loss 0.08589 | ppl     1.09\n",
      "| epoch   1 |   159/  269 batches | lr 0.005000 | 444.79 ms | loss 0.12388 | ppl     1.13\n",
      "| epoch   1 |   212/  269 batches | lr 0.005000 | 434.17 ms | loss 0.05552 | ppl     1.06\n",
      "| epoch   1 |   265/  269 batches | lr 0.005000 | 416.57 ms | loss 0.05051 | ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 120.31s | valid loss 0.11385 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches | lr 0.004802 | 460.81 ms | loss 0.06192 | ppl     1.06\n",
      "| epoch   2 |   106/  269 batches | lr 0.004802 | 446.98 ms | loss 0.07375 | ppl     1.08\n",
      "| epoch   2 |   159/  269 batches | lr 0.004802 | 446.87 ms | loss 0.07587 | ppl     1.08\n",
      "| epoch   2 |   212/  269 batches | lr 0.004802 | 439.00 ms | loss 0.04549 | ppl     1.05\n",
      "| epoch   2 |   265/  269 batches | lr 0.004802 | 348.44 ms | loss 0.05488 | ppl     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 117.26s | valid loss 0.18634 | valid ppl     1.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches | lr 0.004706 | 373.46 ms | loss 0.07197 | ppl     1.07\n",
      "| epoch   3 |   106/  269 batches | lr 0.004706 | 402.74 ms | loss 0.07621 | ppl     1.08\n",
      "| epoch   3 |   159/  269 batches | lr 0.004706 | 407.19 ms | loss 0.07397 | ppl     1.08\n",
      "| epoch   3 |   212/  269 batches | lr 0.004706 | 429.10 ms | loss 0.05464 | ppl     1.06\n",
      "| epoch   3 |   265/  269 batches | lr 0.004706 | 482.42 ms | loss 0.05032 | ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 114.48s | valid loss 0.21031 | valid ppl     1.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches | lr 0.004612 | 432.59 ms | loss 0.12585 | ppl     1.13\n",
      "| epoch   4 |   106/  269 batches | lr 0.004612 | 460.92 ms | loss 0.07821 | ppl     1.08\n",
      "| epoch   4 |   159/  269 batches | lr 0.004612 | 408.17 ms | loss 0.08614 | ppl     1.09\n",
      "| epoch   4 |   212/  269 batches | lr 0.004612 | 378.22 ms | loss 0.05783 | ppl     1.06\n",
      "| epoch   4 |   265/  269 batches | lr 0.004612 | 579.25 ms | loss 0.05248 | ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 123.04s | valid loss 0.24625 | valid ppl     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches | lr 0.004520 | 364.08 ms | loss 0.11789 | ppl     1.13\n",
      "| epoch   5 |   106/  269 batches | lr 0.004520 | 452.60 ms | loss 0.08997 | ppl     1.09\n",
      "| epoch   5 |   159/  269 batches | lr 0.004520 | 371.98 ms | loss 0.08910 | ppl     1.09\n",
      "| epoch   5 |   212/  269 batches | lr 0.004520 | 413.24 ms | loss 0.06313 | ppl     1.07\n",
      "| epoch   5 |   265/  269 batches | lr 0.004520 | 380.56 ms | loss 0.07470 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 108.32s | valid loss 0.23986 | valid ppl     1.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches | lr 0.004429 | 413.42 ms | loss 0.12512 | ppl     1.13\n",
      "| epoch   6 |   106/  269 batches | lr 0.004429 | 354.88 ms | loss 0.13664 | ppl     1.15\n",
      "| epoch   6 |   159/  269 batches | lr 0.004429 | 402.99 ms | loss 0.09801 | ppl     1.10\n",
      "| epoch   6 |   212/  269 batches | lr 0.004429 | 409.67 ms | loss 0.07388 | ppl     1.08\n",
      "| epoch   6 |   265/  269 batches | lr 0.004429 | 427.38 ms | loss 0.07638 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 109.83s | valid loss 0.21701 | valid ppl     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches | lr 0.004341 | 418.74 ms | loss 0.12055 | ppl     1.13\n",
      "| epoch   7 |   106/  269 batches | lr 0.004341 | 521.54 ms | loss 0.13650 | ppl     1.15\n",
      "| epoch   7 |   159/  269 batches | lr 0.004341 | 415.74 ms | loss 0.12082 | ppl     1.13\n",
      "| epoch   7 |   212/  269 batches | lr 0.004341 | 392.77 ms | loss 0.10903 | ppl     1.12\n",
      "| epoch   7 |   265/  269 batches | lr 0.004341 | 434.98 ms | loss 0.10611 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 118.99s | valid loss 0.16737 | valid ppl     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    53/  269 batches | lr 0.004254 | 325.30 ms | loss 0.10427 | ppl     1.11\n",
      "| epoch   8 |   106/  269 batches | lr 0.004254 | 375.08 ms | loss 0.13569 | ppl     1.15\n",
      "| epoch   8 |   159/  269 batches | lr 0.004254 | 383.56 ms | loss 0.13525 | ppl     1.14\n",
      "| epoch   8 |   212/  269 batches | lr 0.004254 | 363.10 ms | loss 0.08551 | ppl     1.09\n",
      "| epoch   8 |   265/  269 batches | lr 0.004254 | 328.79 ms | loss 0.08482 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 97.32s | valid loss 0.28928 | valid ppl     1.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    53/  269 batches | lr 0.004169 | 408.39 ms | loss 0.13280 | ppl     1.14\n",
      "| epoch   9 |   106/  269 batches | lr 0.004169 | 383.55 ms | loss 0.13396 | ppl     1.14\n",
      "| epoch   9 |   159/  269 batches | lr 0.004169 | 387.77 ms | loss 0.11928 | ppl     1.13\n",
      "| epoch   9 |   212/  269 batches | lr 0.004169 | 430.71 ms | loss 0.08885 | ppl     1.09\n",
      "| epoch   9 |   265/  269 batches | lr 0.004169 | 376.73 ms | loss 0.08665 | ppl     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 108.97s | valid loss 0.14766 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    53/  269 batches | lr 0.004085 | 417.15 ms | loss 0.12554 | ppl     1.13\n",
      "| epoch  10 |   106/  269 batches | lr 0.004085 | 428.08 ms | loss 0.14974 | ppl     1.16\n",
      "| epoch  10 |   159/  269 batches | lr 0.004085 | 386.90 ms | loss 0.11876 | ppl     1.13\n",
      "| epoch  10 |   212/  269 batches | lr 0.004085 | 412.50 ms | loss 0.10382 | ppl     1.11\n",
      "| epoch  10 |   265/  269 batches | lr 0.004085 | 424.31 ms | loss 0.10173 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 159.73s | valid loss 0.10973 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    53/  269 batches | lr 0.004004 | 337.56 ms | loss 0.13058 | ppl     1.14\n",
      "| epoch  11 |   106/  269 batches | lr 0.004004 | 394.34 ms | loss 0.12254 | ppl     1.13\n",
      "| epoch  11 |   159/  269 batches | lr 0.004004 | 427.36 ms | loss 0.10594 | ppl     1.11\n",
      "| epoch  11 |   212/  269 batches | lr 0.004004 | 343.74 ms | loss 0.10297 | ppl     1.11\n",
      "| epoch  11 |   265/  269 batches | lr 0.004004 | 368.66 ms | loss 0.11294 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 102.43s | valid loss 0.11089 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    53/  269 batches | lr 0.003924 | 391.94 ms | loss 0.10818 | ppl     1.11\n",
      "| epoch  12 |   106/  269 batches | lr 0.003924 | 366.50 ms | loss 0.16113 | ppl     1.17\n",
      "| epoch  12 |   159/  269 batches | lr 0.003924 | 412.56 ms | loss 0.16630 | ppl     1.18\n",
      "| epoch  12 |   212/  269 batches | lr 0.003924 | 418.82 ms | loss 0.10327 | ppl     1.11\n",
      "| epoch  12 |   265/  269 batches | lr 0.003924 | 561.93 ms | loss 0.10758 | ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 117.57s | valid loss 0.15332 | valid ppl     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    53/  269 batches | lr 0.003845 | 422.50 ms | loss 0.10686 | ppl     1.11\n",
      "| epoch  13 |   106/  269 batches | lr 0.003845 | 384.18 ms | loss 0.14213 | ppl     1.15\n",
      "| epoch  13 |   159/  269 batches | lr 0.003845 | 404.82 ms | loss 0.17680 | ppl     1.19\n",
      "| epoch  13 |   212/  269 batches | lr 0.003845 | 490.77 ms | loss 0.09161 | ppl     1.10\n",
      "| epoch  13 |   265/  269 batches | lr 0.003845 | 458.55 ms | loss 0.11305 | ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 118.80s | valid loss 0.10861 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "input_window = 100\n",
    "output_window = 5\n",
    "batch_size = 10 # batch size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "       \n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "def get_data():\n",
    "    # time        = np.arange(0, 400, 0.1)\n",
    "    # amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
    "    \n",
    "    from pandas import read_csv\n",
    "    series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True)\n",
    "    # series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    # amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    \n",
    "    sampels = 2800\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment.. \n",
    "    train_sequence = create_inout_sequences(train_data,input_window)\n",
    "    train_sequence = train_sequence[:-output_window] #todo: fix hack?\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,input_window)\n",
    "    test_data = test_data[:-output_window] #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device),test_data.to(device)\n",
    "\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i,batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)        \n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "    pyplot.plot(test_result-truth,color=\"green\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "    \n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.close()\n",
    "        \n",
    "# entweder ist hier ein fehler im loss oder in der train methode, aber die ergebnisse sind unterschiedlich \n",
    "# auch zu denen der predict_future\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).cpu().item()            \n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "train_data, val_data = get_data()\n",
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    \n",
    "    if(epoch % 10 is 0):\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() \n",
    "\n",
    "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number) \n",
    "#out = model(src)\n",
    "#\n",
    "#print(out)\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f8c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
